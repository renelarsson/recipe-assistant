{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1919fb56",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) for Recipe Assistant: RAG Evaluation\n",
    "\n",
    "This notebook provides an evaluation framework for the RAG system described in `rag-flow.ipynb`.  \n",
    "It covers ground-truth generation, retrieval metrics (Hit Rate, MRR), parameter optimization, and LLM-as-judge answer quality for both Minsearch and Elasticsearch using the best-performing combined retrieval and reranking approaches.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db931079",
   "metadata": {},
   "source": [
    "### 1. Set-up Dependencies, OpenAI Client and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d6aa1",
   "metadata": {},
   "source": [
    "#### 1.1 Dependencies and OpenAI Client\n",
    "Import dependencies, load OpenAI API key and connect to OpenAI API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91136bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import dotenv\n",
    "import minsearch\n",
    "from elasticsearch import Elasticsearch\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a93e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(\"../.env\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d5858",
   "metadata": {},
   "source": [
    "#### 1.2. Load and Index Recipe Data\n",
    "Load the recipe dataset from CSV file and prepare it for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad929e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe_name': 'Spaghetti Carbonara',\n",
       " 'cuisine_type': 'Italian',\n",
       " 'meal_type': 'Dinner',\n",
       " 'difficulty_level': 'Medium',\n",
       " 'prep_time_minutes': 15,\n",
       " 'cook_time_minutes': 20,\n",
       " 'servings': 4,\n",
       " 'main_ingredients': 'Spaghetti, Eggs, Pancetta, Parmesan',\n",
       " 'all_ingredients': 'Spaghetti, Eggs, Pancetta, Parmesan, Black Pepper, Olive Oil, Salt',\n",
       " 'dietary_restrictions': 'Contains gluten, dairy, pork',\n",
       " 'instructions': 'Boil spaghetti until al dente. Fry pancetta until crispy. Whisk eggs with grated Parmesan. Toss hot spaghetti with pancetta and egg mixture off heat. Serve immediately with black pepper.',\n",
       " 'nutritional_info': 'Calories: 520, Protein: 22g, Carbs: 60g, Fat: 22g',\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/recipes_clean.csv')\n",
    "\n",
    "# Add an ID column if it doesn't exist\n",
    "if 'id' not in df.columns:\n",
    "    df['id'] = range(len(df))\n",
    "    \n",
    "# Create documents for indexing\n",
    "documents = df.to_dict(orient='records')\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fa8542",
   "metadata": {},
   "source": [
    "### 2. Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a98abf",
   "metadata": {},
   "source": [
    "#### 2.1 Create Time Filter\n",
    "Users will be able to filter recipes by maximum preparation and cooking time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e736acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_max_time(results, max_time=None):\n",
    "    if max_time is None:\n",
    "        return results\n",
    "    filtered = []\n",
    "    for doc in results:\n",
    "        try:\n",
    "            total_time = int(doc.get('prep_time_minutes', 0)) + int(doc.get('cook_time_minutes', 0))\n",
    "        except Exception:\n",
    "            total_time = 99999\n",
    "        if total_time <= max_time:\n",
    "            filtered.append(doc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66fd2b1",
   "metadata": {},
   "source": [
    "#### 2.2. Deduplicate Search results\n",
    "Elasticsearch and LLMs can sometimes return duplicate or near-duplicate documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe61132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_results(results):\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for doc in results:\n",
    "        # Use a unique field, e.g. 'id' or a tuple of fields\n",
    "        key = doc.get('id') or (doc.get('recipe_name'), doc.get('prep_time_minutes'), doc.get('cook_time_minutes'))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(doc)\n",
    "    return deduped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f061a",
   "metadata": {},
   "source": [
    "### 3. Ground Truth Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5606a13",
   "metadata": {},
   "source": [
    "#### 3.1. Generate ground-truth user questions for each recipe using the LLM.  \n",
    "This is used to evaluate retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a0d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb6f903bf8a487f8b5a42de31208ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The fields are the most relevant for generating realistic user questions, given the prompt instructions.\n",
    "prompt_template = \"\"\"\n",
    "You are emulating a user of a recipe assistant.\n",
    "Given the following recipe record, generate 5 realistic user queries that could be answered by retrieving this recipe.\n",
    "Each query should be phrased as if a user is searching for a recipe to cook, mentioning available ingredients and/or time constraints.\n",
    "- At least 3 queries must mention specific ingredient combinations (e.g., \"What can I cook with eggs, pancetta, and parmesan?\").\n",
    "- At least 2 queries must mention a time constraint (e.g., \"I need a dinner recipe that takes less than 40 minutes.\").\n",
    "- Do NOT mention the recipe name or copy long phrases from the record.\n",
    "- Paraphrase naturally and vary the style of the queries.\n",
    "- Each query should be answerable using only the information in the recipe record.\n",
    "\n",
    "The recipe record:\n",
    "Recipe Name: {recipe_name}\n",
    "Cuisine: {cuisine_type}\n",
    "Meal Type: {meal_type}\n",
    "Difficulty: {difficulty_level}\n",
    "Prep Time: {prep_time_minutes} minutes\n",
    "Cook Time: {cook_time_minutes} minutes\n",
    "Servings: {servings}\n",
    "Main Ingredients: {main_ingredients}\n",
    "All Ingredients: {all_ingredients}\n",
    "Dietary Restrictions: {dietary_restrictions}\n",
    "Instructions: {instructions}\n",
    "Nutritional Info: {nutritional_info}\n",
    "\n",
    "Return the output as valid JSON (no code block), in this format:\n",
    "{{\"queries\": [\"query1\", \"query2\", \"query3\", \"query4\", \"query5\"]}}\n",
    "\"\"\".strip()\n",
    "    \n",
    "def generate_queries(doc):\n",
    "    \"\"\"\n",
    "    Fills in the template with the recipe fields.\n",
    "    Sends the prompt to the LLM (OpenAI API).\n",
    "    Receives the LLM’s response (a JSON string with the queries).\n",
    "    \"\"\"\n",
    "    prompt = prompt_template.format(**doc)\n",
    "    # Parse the LLM’s response to extract the questions\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "results = {}\n",
    "for i, doc in enumerate(tqdm(documents[:100])):\n",
    "    doc_id = doc.get('id', i)\n",
    "    if doc_id in results:\n",
    "        continue\n",
    "    try:\n",
    "        queries_raw = generate_queries(doc)\n",
    "        # Extract the questions from the LLM’s output\n",
    "        queries = json.loads(queries_raw)\n",
    "        results[doc_id] = queries['queries']\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        continue\n",
    "\n",
    "final_results = []\n",
    "for doc_id, queries in results.items():\n",
    "    for q in queries:\n",
    "        final_results.append((doc_id, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d83c1e",
   "metadata": {},
   "source": [
    "#### 3.2. Save and Re-load Ground Truth Questions\n",
    "\n",
    "Load the generated ground-truth questions for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26917b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results = pd.DataFrame(final_results, columns=['id', 'question'])\n",
    "#df_results.to_csv('../data/ground-truth-retrieval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8eb9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt = pd.read_csv('../data/ground-truth-retrieval.csv')\n",
    "ground_truth = df_gt.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24269e97",
   "metadata": {},
   "source": [
    "### 4. Minsearch Setup and Retrieval Strategies\n",
    "Set up Minsearch retrieval backend as in the main RAG notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448c394",
   "metadata": {},
   "source": [
    "#### 4.1. Minsearch Index \n",
    "\n",
    "Prep/cook time and servings are numeric, and not suitable for Minsearch's default text/keyword search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=['recipe_name', 'main_ingredients', 'all_ingredients', \\\n",
    "                 'instructions', 'cuisine_type', 'dietary_restrictions'],\n",
    "    keyword_fields=['meal_type', 'difficulty_level'] # Categorical values\n",
    ")\n",
    "\n",
    "# Fit/train the index on the recipe documents\n",
    "index.fit(documents)\n",
    "index.documents = index.docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04217f85",
   "metadata": {},
   "source": [
    "#### 4.2. Generate an Embedding Vector (the user's search input at runtime)\n",
    "Combines keyword and with OpenAI's embedding-based (semantic) similarity for more robust retrieval. index.documents (or index.docs) is a list of recipe dictionaries (the original documents). However we need to generate embeddings (e.g., with OpenAI), and add them to each document under the all_ingredients_vector field before indexing. The resulting index.embeddings is a list/array of vectors, one per recipe, in the same order as index.documents (the same as the documents list). These embeddings are used for semantic search (vector similarity/cosineSimilarity) in both Minsearch and Elasticsearch retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab89928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    response = client.embeddings.create( # OpenAI client\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=[text]\n",
    "    )\n",
    "    return np.array(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8fad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Precompute embeddings for all recipes if not saved in rag-flow.ipynb \n",
    "# DO THIS ONCE AND CACHE: Only rerun if changes to the recipe_clean.csv dataset is made \n",
    "if not hasattr(index, \"embeddings\"):\n",
    "    index.embeddings = [get_embedding(doc['all_ingredients']) for doc in index.documents]\n",
    "np.save('embeddings.npy', np.array(index.embeddings))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5dc7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from file for future use/new notebook session\n",
    "index.embeddings = np.load('embeddings.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f2344",
   "metadata": {},
   "source": [
    "#### 4.3. Retrieval Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c04f2eb",
   "metadata": {},
   "source": [
    "##### Approach 5: Cover Ingredients Search\n",
    "Selects a set of recipes that together cover as many of the query ingredients as possible, optionally filtering out recipes that exceed the max_time constraint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbfa3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ingredients(ingredient_str):\n",
    "    # Split only on commas, strip whitespace, and lowercase\n",
    "    return set([ing.strip().lower() for ing in ingredient_str.split(',') if ing.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b924202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_cover_ingredients_search(query, index, num_results=5, max_time=None):\n",
    "    \"\"\"\n",
    "    Selects a set of recipes that together cover as many of the query ingredients as possible,\n",
    "    optionally filtering out recipes that exceed the max_time constraint.\n",
    "    \"\"\"\n",
    "    query_ings = tokenize_ingredients(query)\n",
    "    uncovered = set(query_ings)\n",
    "    selected = []\n",
    "    docs = index.documents.copy() \n",
    "\n",
    "    # Apply time filter to docs at the start for efficiency\n",
    "    docs = filter_by_max_time(docs, max_time)\n",
    "\n",
    "    while uncovered and len(selected) < num_results and docs:\n",
    "        best_doc = None\n",
    "        best_overlap = 0\n",
    "        for doc in docs:\n",
    "            recipe_ings = tokenize_ingredients(doc.get('all_ingredients', ''))\n",
    "            overlap = len(uncovered & recipe_ings)\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_doc = doc\n",
    "        if best_doc and best_overlap > 0:\n",
    "            selected.append(best_doc)\n",
    "            recipe_ings = tokenize_ingredients(best_doc.get('all_ingredients', ''))\n",
    "            uncovered -= recipe_ings\n",
    "            docs.remove(best_doc)\n",
    "        else:\n",
    "            break\n",
    "    # Deduplicate results before returning (recommended if docs may have overlap)\n",
    "    deduped_results = deduplicate_results(selected)\n",
    "    return deduped_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc586420",
   "metadata": {},
   "source": [
    "#### Combined Pipeline Approach: Combine Cover and Hybrid RAG Pipelines (Minsearch)\n",
    "\n",
    "Combine the two approaches by first running Cover Ingredients Search to select a diverse set of recipes that cover as many ingredients as possible, and then passing those results through the Hybrid Search to rerank or filter them by semantic relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2667098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_cover_then_hybrid_search(query, index, num_results=5, max_time=None, hybrid_top_k=5):\n",
    "    \"\"\"\n",
    "    Combine Cover Ingredients Search and Hybrid Search:\n",
    "    1. Run Cover Ingredients Search to get a diverse set of recipes (pool size = num_results*4).\n",
    "    2. Rerank the pool by semantic similarity to the query using OpenAI embeddings.\n",
    "    3. Deduplicate results before returning.\n",
    "    \"\"\"\n",
    "    # Step 1: Cover Ingredients Search to get a diverse set and increase the pool size\n",
    "    cover_results = ms_cover_ingredients_search(query, index, num_results=num_results*4, max_time=max_time)\n",
    "    if not cover_results:\n",
    "        return []\n",
    "    # Step 2: Hybrid Search, but restrict to cover_results as the candidate pool\n",
    "    # Compute embedding for the query\n",
    "    query_emb = get_embedding(query)\n",
    "    # Compute similarities only for cover_results using cached embeddings\n",
    "    cover_embeddings = [\n",
    "        index.embeddings[index.documents.index(doc)]\n",
    "        for doc in cover_results\n",
    "    ]\n",
    "    similarities = [np.dot(query_emb, emb) for emb in cover_embeddings]\n",
    "    # Get top-k by semantic similarity\n",
    "    top_indices = np.argsort(similarities)[-hybrid_top_k:][::-1]\n",
    "    # The hybrid logic is built into the similarity ranking (symbolic/keyword + neural/embedding/semantic)\n",
    "    hybrid_results = [cover_results[i] for i in top_indices]\n",
    "    # Deduplicate before returning (defensive, in case of overlap)\n",
    "    deduped_results = deduplicate_results(hybrid_results[:num_results])\n",
    "    return deduped_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d752ee",
   "metadata": {},
   "source": [
    "### 5. Elasticsearch Setup and Retrieval Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05928346",
   "metadata": {},
   "source": [
    "#### 5.1 Set up Elasticsearch Docker and Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6befe638",
   "metadata": {},
   "source": [
    "---\n",
    "Run docker in terminal:\n",
    "\n",
    "`docker run -d --name elasticsearch -p 9200:9200 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:8.13.4`\n",
    "\n",
    "Or:\n",
    "\n",
    "`docker run -d --name elasticsearch \\\n",
    "  -p 9200:9200 \\\n",
    "  -e \"discovery.type=single-node\" \\\n",
    "  -e \"xpack.security.enabled=false\" \\\n",
    "  -e \"ES_JAVA_OPTS=-Xms512m -Xmx1g\" \\\n",
    "  docker.elastic.co/elasticsearch/elasticsearch:8.13.4`\n",
    "\n",
    "And check if Elasticsearch is up:\n",
    "\n",
    "`curl http://localhost:9200`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba1a22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Elasticsearch Client (make sure Elasticsearch is running locally)\n",
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc08e242",
   "metadata": {},
   "source": [
    "#### 5.2. Elasticsearch Index \n",
    "Set up Elasticsearch retrieval backend as in the main RAG notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b513183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index may already exist: BadRequestError(400, 'resource_already_exists_exception', 'index [recipes/RWTKgx2XR_KP-wnMuWG2mQ] already exists')\n"
     ]
    }
   ],
   "source": [
    "# Define Elasticsearch index settings and mappings for recipes\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1, # A unit of storage and search. More shards can improve parallelism for large datasets\n",
    "        \"number_of_replicas\": 0 # A shard copy for fault tolerance and increased search throughput (not recommended for production)\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"recipe_name\": {\"type\": \"text\"},\n",
    "            \"main_ingredients\": {\"type\": \"text\"},\n",
    "            \"all_ingredients\": {\"type\": \"text\"},\n",
    "            \"instructions\": {\"type\": \"text\"},\n",
    "            \"cuisine_type\": {\"type\": \"text\"},\n",
    "            \"dietary_restrictions\": {\"type\": \"text\"},\n",
    "            \"meal_type\": {\"type\": \"keyword\"},\n",
    "            \"difficulty_level\": {\"type\": \"keyword\"},\n",
    "            \"prep_time_minutes\": {\"type\": \"integer\"},\n",
    "            \"cook_time_minutes\": {\"type\": \"integer\"},\n",
    "            \"all_ingredients_vector\": { # Define a dense vector field\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 1536  # text-embedding-3-small returns 1536-dimensional vectors\n",
    "}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"recipes\"\n",
    "\n",
    "# Create the index (ignore error if it already exists)\n",
    "try:\n",
    "    es_client.indices.create(index=index_name, body=index_settings)\n",
    "except Exception as e:\n",
    "    print(\"Index may already exist:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a11b7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the embedding vector (all_ingredients_vector) to each document when indexing\n",
    "for doc, emb in zip(documents, index.embeddings): # Reuse the same embeddings for Elasticsearch, since order matches the documents list\n",
    "    doc['all_ingredients_vector'] = emb.tolist()\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c905e4",
   "metadata": {},
   "source": [
    "#### 5.3 Run RAG Retrieval Strategies for Evaluation (Elasticsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5e53a",
   "metadata": {},
   "source": [
    "##### Approach 1: Basic Keyword Search\n",
    "\n",
    "Returns recipes that match the query using simple keyword matching across all fields. **Will be used in cover strategy simulation for Elasticsearch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e108a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic keyword-based search using Elasticsearch's multi_match query\n",
    "def es_basic_search(query, num_results=5, max_time=None):\n",
    "    search_query = {\n",
    "        \"size\": num_results * 2,  # get more to allow filtering\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [ # Elasticsearch's multi_match query is for text searching\n",
    "                    \"recipe_name\",\n",
    "                    \"main_ingredients^2\",\n",
    "                    \"all_ingredients^3\",\n",
    "                    \"instructions^1.5\",\n",
    "                    \"cuisine_type\",\n",
    "                    \"dietary_restrictions^1.5\"\n",
    "                ],\n",
    "                \"type\": \"best_fields\" # default, can be \"most_fields\", \"cross_fields\", etc.\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    result_docs = [hit['_source'] for hit in response['hits']['hits']]\n",
    "    # Apply time filter\n",
    "    if max_time is not None:\n",
    "        result_docs = filter_by_max_time(result_docs, max_time)\n",
    "    return result_docs[:num_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97289f38",
   "metadata": {},
   "source": [
    "##### Approach 5: Cover Ingredients Search\n",
    "Elasticsearch queries return a ranked list based on scoring, but do not natively support iterative, set-cover-style selection across multiple results. A workaround is to retrieve many candidates from Elasticsearch, then run the cover algorithm in Python on those results. This function does not use embeddings or vector similarity. A cover-style retrieval strategy uses only keyword search to get a candidate pool from Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4d8d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_cover_ingredients_search(query, num_results=5, max_time=None, candidate_pool_size=200):\n",
    "    \"\"\"\n",
    "    Simulate Cover Ingredients Search using Elasticsearch results, with optional time filtering.\n",
    "    Returns a set of recipes that together cover as many of the query ingredients as possible.\n",
    "    Deduplicates results before returning.\n",
    "    \"\"\"\n",
    "    # Step 1: Get a large pool of candidates from ES (basic search, large pool)\n",
    "    candidates = es_basic_search(query, num_results=candidate_pool_size)\n",
    "    # Step 2: Filter by time if needed\n",
    "    if max_time is not None:\n",
    "        candidates = filter_by_max_time(candidates, max_time)\n",
    "    # Step 3: Apply greedy cover algorithm\n",
    "    query_tokens = set(re.sub(r'[^\\w\\s]', '', query.lower()).replace(',', ' ').split())\n",
    "    uncovered = set(query_tokens)\n",
    "    selected = []\n",
    "    docs = candidates.copy()\n",
    "    while uncovered and len(selected) < num_results and docs:\n",
    "        best_doc = None\n",
    "        best_overlap = 0\n",
    "        for doc in docs:\n",
    "            ingredients = set(re.sub(r'[^\\w\\s]', '', str(doc.get('all_ingredients', '')).lower()).replace(',', ' ').split())\n",
    "            overlap = len(uncovered & ingredients)\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_doc = doc\n",
    "        if best_doc and best_overlap > 0:\n",
    "            selected.append(best_doc)\n",
    "            ingredients = set(re.sub(r'[^\\w\\s]', '', str(best_doc.get('all_ingredients', '')).lower()).replace(',', ' ').split())\n",
    "            uncovered -= ingredients\n",
    "            docs.remove(best_doc)\n",
    "        else:\n",
    "            break\n",
    "    # Deduplicate results before returning (recommended if docs may have overlap)\n",
    "    deduped_results = deduplicate_results(selected)\n",
    "    return deduped_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e19f8",
   "metadata": {},
   "source": [
    "#### Combined Pipeline Approach: Combine Cover and Hybrid RAG Pipelines (Elasticsearch)\n",
    "\n",
    "Combine the two approaches by first running Cover Ingredients Search to select a diverse set of recipes that cover as many ingredients as possible, and then passing those results through the Hybrid Search to rerank or filter them by semantic relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbba3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_cover_then_hybrid_search(query, num_results=5, max_time=None, hybrid_top_k=5, candidate_pool_size=200):\n",
    "    \"\"\"\n",
    "    Combine Cover Ingredients Search and Hybrid Search for Elasticsearch:\n",
    "    1. Run Cover Ingredients Search to get a diverse set of recipes (pool size = candidate_pool_size).\n",
    "    2. Rerank the pool by semantic similarity to the query using OpenAI embeddings.\n",
    "    3. Remove 'all_ingredients_vector' from each result for cleaner output.\n",
    "    4. Deduplicate results before returning.\n",
    "    \"\"\"\n",
    "    # Step 1: Cover Ingredients Search to get a diverse set and increase the pool size\n",
    "    cover_results = es_cover_ingredients_search(query, num_results=candidate_pool_size, max_time=max_time)\n",
    "    if not cover_results:\n",
    "        return []\n",
    "    # Step 2: Hybrid Search, but restrict to cover_results as the candidate pool\n",
    "    query_emb = get_embedding(query)\n",
    "    # Use precomputed embeddings from 'all_ingredients_vector'\n",
    "    cover_embeddings = [\n",
    "        np.array(doc['all_ingredients_vector']) for doc in cover_results\n",
    "    ]\n",
    "    similarities = [np.dot(query_emb, emb) for emb in cover_embeddings]\n",
    "    # Get top-k by semantic similarity\n",
    "    top_indices = np.argsort(similarities)[-hybrid_top_k:][::-1]\n",
    "    # The hybrid logic is built into the similarity ranking (symbolic/keyword + neural/embedding/semantic)\n",
    "    hybrid_results = [cover_results[i] for i in top_indices] \n",
    "    # Remove all_ingredients_vector from each result\n",
    "    for doc in hybrid_results:\n",
    "        doc.pop('all_ingredients_vector', None)\n",
    "\n",
    "    return hybrid_results[:num_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca3585",
   "metadata": {},
   "source": [
    "### 6. Rerank with LLM\n",
    "\n",
    "Re-ranking means taking an initial set of retrieved documents (candidates) and re-ordering them, often using a more sophisticated model (like an LLM), to improve the relevance of the top results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae858d",
   "metadata": {},
   "source": [
    "#### 6.1. Build Prompt for the LLM Using Retrieved Context (Minsearch + Elasticsearch)\n",
    "Use OpenAI's language model to generate answers based on the context retrieved from Minsearch and Elasticsearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fc8dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    entry_template = \"\"\"\n",
    "Recipe: {recipe_name}\n",
    "Cuisine: {cuisine_type}\n",
    "Meal Type: {meal_type}\n",
    "Difficulty: {difficulty_level}\n",
    "Prep Time: {prep_time_minutes} minutes\n",
    "Cook Time: {cook_time_minutes} minutes\n",
    "Main Ingredients: {main_ingredients}\n",
    "Instructions: {instructions}\n",
    "Dietary Info: {dietary_restrictions}\n",
    "\"\"\".strip()\n",
    "    context = \"\\n\\n\".join([entry_template.format(**doc) for doc in search_results])\n",
    "    prompt_template = \"\"\"\n",
    "You are an expert chef and culinary assistant. Answer the question based on the content from our recipe database.\n",
    "Use only the facts from the context when answering the question.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide recipe recommendations with brief explanations of why they match the requested ingredients.\n",
    "If exact ingredients aren't available, suggest the closest matches and mention any substitutions needed.\n",
    "\"\"\".strip()\n",
    "    return prompt_template.format(context=context, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4212e",
   "metadata": {},
   "source": [
    "#### 6.2. LLM Call Function (Minsearch + Elasticsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc4cd25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b69e5",
   "metadata": {},
   "source": [
    "#### 6.3. Re-ranking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3162012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_llm(query, candidates, max_time=None): \n",
    "    # Apply time filter before reranking\n",
    "    if max_time is not None:\n",
    "        candidates = filter_by_max_time(candidates, max_time)\n",
    "    context = \"\\n\\n\".join([f\"Recipe: {doc['recipe_name']}\\nIngredients: {doc['main_ingredients']}\" for doc in candidates])\n",
    "    prompt = f\"\"\"\n",
    "Given the following user query and candidate recipes, rank the recipes from most to least relevant.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Candidates:\n",
    "{context}\n",
    "\n",
    "Return a JSON list of recipe names in ranked order.\n",
    "\"\"\".strip()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    import re\n",
    "    content = response.choices[0].message.content\n",
    "    json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
    "    if json_match:\n",
    "        ranked_names = json.loads(json_match.group())\n",
    "    else:\n",
    "        return candidates\n",
    "    ranked_docs = [doc for name in ranked_names for doc in candidates if doc['recipe_name'] == name]\n",
    "    return ranked_docs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b563c",
   "metadata": {},
   "source": [
    "#### 6.4. Re-rank the ms_cover_then_hybrid_search RAG Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_best_rag_with_rerank(query):\n",
    "    candidates = ms_cover_then_hybrid_search(query, index, num_results=5)\n",
    "    reranked = rerank_with_llm(query, candidates)\n",
    "    prompt = build_prompt(query, reranked)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae80298",
   "metadata": {},
   "source": [
    "#### 6.5. Re-rank the es_cover_then_hybrid_search RAG Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ee268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_best_rag_with_rerank(query):\n",
    "    candidates = es_cover_then_hybrid_search(query, num_results=5)\n",
    "    reranked = rerank_with_llm(query, candidates)\n",
    "    prompt = build_prompt(query, reranked)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c04b39",
   "metadata": {},
   "source": [
    "#### 6.6. Run and Compare Both Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927791e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minsearch Best RAG with LLM Rerank:\n",
      "Based on the requested ingredients, here are the closest recipe recommendations along with explanations:\n",
      "\n",
      "1. **Mediterranean Rice Skillet**\n",
      "   - **Matching Ingredients:** Rice, Eggs, Cheese, Garlic, Onion\n",
      "   - **Explanation:** This recipe primarily utilizes rice, which is one of your requested ingredients. It also includes eggs and cheese, which match your list. Garlic and onion add flavor, aligning with the garlic and onion you requested. However, it does not include tomatoes or bell peppers. You could add chopped bell peppers as a substitution to enhance the dish.\n",
      "\n",
      "2. **Chinese Pasta Soup**\n",
      "   - **Matching Ingredients:** Pasta, Shrimp, Garlic, Milk (for butter)\n",
      "   - **Explanation:** This recipe uses pasta and shrimp, matching your list. While it does not include chicken, eggs, or rice, you could consider substituting pasta with rice if desired. The soup's creamy texture is achieved with milk and butter, which can add richness to your meal.\n",
      "\n",
      "3. **Beef Tacos**\n",
      "   - **Matching Ingredients:** Tortillas (potentially flour), Garlic (if added in the seasoning), Tomato\n",
      "   - **Explanation:** While the primary ingredient here is ground beef, tortillas made from flour would incorporate into your ingredient list, provided you opt for flour tortillas. You could add tomato for a fresh topping, aligning with your request. To stay closer to your ingredient preferences, consider incorporating cheese or a side of rice.\n",
      "\n",
      "4. **Indian Tofu Cookies**\n",
      "   - **Closest Matches:** While this recipe primarily has tofu, chicken, and potatoes, it includes sugar and butter.\n",
      "   - **Explanation:** This recipe employs sugar and butter, which match your criteria. However, it doesn’t feature rice, pasta, or other requested items. If you are looking for a dessert option, this might work well, though it may require a protein swap (tofu instead of chicken) and does not use all the requested ingredients.\n",
      "\n",
      "**Note**: Among the recipes, the Mediterranean Rice Skillet aligns the closest overall by using multiple ingredients you requested, making it a solid recommendation. Adjustments and additions can enhance any of these recipes to cater specifically to your tastes and the ingredient list provided.\n",
      "\n",
      "Elasticsearch Best RAG with LLM Rerank:\n",
      "Based on the ingredients you listed, here are the recipe recommendations from the database, along with explanations and potential substitutions:\n",
      "\n",
      "1. **Mediterranean Rice Skillet**\n",
      "   - **Matching Ingredients:** Rice, Cheese, Eggs, Garlic, Onion\n",
      "   - **Explanation:** This recipe includes rice, cheese, eggs, garlic, and onion, which are part of your ingredient list. You can use the rice and eggs as central ingredients and the garlic and onion for seasoning. Although flour is in the instructions, it is not a main ingredient; however, you can incorporate some if desired. \n",
      "   - **Substitutions:** If you don't have rice, you could potentially use pasta as a substitute, although it would change the dish's overall character.\n",
      "\n",
      "2. **Chinese Pasta Soup**\n",
      "   - **Matching Ingredients:** Pasta, Shrimp, Garlic, Milk\n",
      "   - **Explanation:** This breakfast recipe features pasta, shrimp, garlic, and milk. It matches your list well with a focus on shrimp, which is a key ingredient. \n",
      "   - **Substitutions:** If you want to include chicken instead of shrimp, that would work as a substitution, but it would alter the flavor profile significantly.\n",
      "\n",
      "3. **French Fish Soup**\n",
      "   - **Matching Ingredients:** Fish, Potatoes, Rice, Garlic, Milk\n",
      "   - **Explanation:** In this soup, you will find fish and potatoes among the ingredients (rice is also included). This recipe makes use of garlic for seasoning, aligning it with your ingredient list.\n",
      "   - **Substitutions:** If you don't have fish, you could consider using chicken instead. \n",
      "\n",
      "4. **African Chicken Casserole**\n",
      "   - **Matching Ingredients:** Chicken, Cheese, Garlic\n",
      "   - **Explanation:** If you have chicken, this dish works perfectly since it includes chicken and cheese, with garlic for flavoring. This could be a delightful savory snack.\n",
      "   - **Substitutions:** You may need to omit fish if you choose to make this dish.\n",
      "\n",
      "5. **French Onion Soup**\n",
      "   - **Matching Ingredients:** Onion, (possibly Cheese if you have Gruyere or similar)\n",
      "   - **Explanation:** While this recipe primarily focuses on onions and cheese, it could incorporate garlic as well, which you have listed. \n",
      "   - **Substitutions:** If you need to include a protein or a substantial side, consider serving this broth-based soup with a side of rice or pasta to align it more closely with your ingredient list.\n",
      "\n",
      "### Summary of Closest Matches:\n",
      "- **Rice has a prominent role in Mediterranean dishes.**\n",
      "- **Pasta fits with the Chinese Pasta Soup.**\n",
      "- **Chicken can be easily incorporated into various recipes, especially in the African Chicken Casserole.**\n",
      "\n",
      "Always make sure to adjust seasoning and cooking times as necessary when making substitutions. Happy cooking!\n"
     ]
    }
   ],
   "source": [
    "ingredients = \"tomato, flour, sugar, chicken, rice, butter, chocolate, shrimp, potatoes, eggs, milk, pasta, cheese, garlic, onion, bell pepper, fish, lemon, thyme\"\n",
    "\n",
    "print(\"Minsearch Best RAG with LLM Rerank:\")\n",
    "answer_ms = ms_best_rag_with_rerank(ingredients)\n",
    "print(answer_ms)\n",
    "\n",
    "print(\"\\nElasticsearch Best RAG with LLM Rerank:\")\n",
    "answer_es = es_best_rag_with_rerank(ingredients)\n",
    "print(answer_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6af353",
   "metadata": {},
   "source": [
    "#### 6.7. Conclusion of Combined RAG Pipeline with Re-ranking for Minsearch and Elasticsearch\n",
    "\n",
    "1. Both pipelines surface similar top matches.\n",
    "\n",
    "    * Both recommend Mediterranean Rice Skillet and Chinese Pasta Soup as strong matches for the ingredient list.\n",
    "    * French Onion Soup appears in both lists, though not always as a top match.\n",
    "\n",
    "2. The explanations are ingredient-aware and suggest substitutions.\n",
    "\n",
    "    * Both systems explain which ingredients from the list match the recipes.\n",
    "    * They offer suggestions for substitutions or adaptations (e.g., using chicken instead of beef, or substituting fish).\n",
    "\n",
    "3. Elasticsearch results are slightly more ingredient-dense and explicit.\n",
    "\n",
    "    * The Elasticsearch output lists more matching ingredients for each recipe and sometimes provides more detailed substitution advice.\n",
    "    * It also suggests how to use leftover ingredients (e.g., lemon, chocolate) in side dishes or desserts.\n",
    "\n",
    "4. Both systems handle partial matches and adaptation.\n",
    "\n",
    "    * Recipes that don’t fully match the list are still suggested, with clear notes on what’s missing and how to adapt.\n",
    "    * Both pipelines are robust to imperfect matches and provide practical suggestions.\n",
    "\n",
    "5. The LLM reranker produces user-friendly, actionable recommendations.\n",
    "\n",
    "    * The final output is readable, helpful, and tailored to the user’s pantry.\n",
    "    * The explanations help users understand why each recipe was chosen and how to adapt it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e3752",
   "metadata": {},
   "source": [
    "### 8. Prompt Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8cbd7c",
   "metadata": {},
   "source": [
    "#### 8.1. Compare Different Prompt Strategies for Minsearch and Elasticsearch RAG pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02d7311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_strategy_1():\n",
    "    return \"\"\"\n",
    "You are a chef assistant. Based on the available recipes, recommend dishes that use the requested ingredients.\n",
    "Provide the recipe name, brief description, and cooking instructions and time.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "def get_prompt_strategy_2():\n",
    "    return \"\"\"\n",
    "You are an expert chef specializing in ingredient substitutions. When users provide ingredients,\n",
    "recommend recipes and suggest alternatives for missing ingredients. Always explain possible substitutions\n",
    "and how they might affect the dish.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide recommendations with substitution suggestions:\n",
    "\"\"\".strip()\n",
    "\n",
    "def get_prompt_strategy_3():\n",
    "    return \"\"\"\n",
    "You are a nutritionist and chef. Recommend recipes based on ingredients provided, considering\n",
    "nutritional value and dietary restrictions. Highlight health benefits and suggest modifications\n",
    "for different dietary needs (vegetarian, gluten-free, etc.).\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide nutritionally-aware recommendations:\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc7d808",
   "metadata": {},
   "source": [
    "#### 8.2. Prompt-strategy version (flexible prompt template)\n",
    "\n",
    "Prompt-strategies enable experimentation with different prompt styles, tones, or instructions. You can test how changing the prompt wording or focus (e.g., substitutions, nutrition, chef persona) affects the quality and usefulness of answers. This is valuable for prompt engineering, user experience research, and optimizing the assistant for different use cases or audiences.\n",
    "See fixed version below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_minsearch(question, prompt_strategy=None):\n",
    "    search_results = ms_cover_then_hybrid_search(question, index, num_results=5)\n",
    "    # Build context string as in build_prompt\n",
    "    entry_template = (\n",
    "        \"Recipe: {recipe_name}\\n\"\n",
    "        \"Cuisine: {cuisine_type}\\n\"\n",
    "        \"Meal Type: {meal_type}\\n\"\n",
    "        \"Difficulty: {difficulty_level}\\n\"\n",
    "        \"Prep Time: {prep_time_minutes} minutes\\n\"\n",
    "        \"Cook Time: {cook_time_minutes} minutes\\n\"\n",
    "        \"Main Ingredients: {main_ingredients}\\n\"\n",
    "        \"Instructions: {instructions}\\n\"\n",
    "        \"Dietary Info: {dietary_restrictions}\"\n",
    "    )\n",
    "    context = \"\\n\\n\".join([entry_template.format(**doc) for doc in search_results])\n",
    "    prompt = prompt_strategy().format(context=context, question=question)\n",
    "    answer = llm(prompt)\n",
    "    return answer\n",
    "\n",
    "def rag_elasticsearch(question, prompt_strategy=None):\n",
    "    search_results = es_cover_then_hybrid_search(question, num_results=5)\n",
    "    entry_template = (\n",
    "        \"Recipe: {recipe_name}\\n\"\n",
    "        \"Cuisine: {cuisine_type}\\n\"\n",
    "        \"Meal Type: {meal_type}\\n\"\n",
    "        \"Difficulty: {difficulty_level}\\n\"\n",
    "        \"Prep Time: {prep_time_minutes} minutes\\n\"\n",
    "        \"Cook Time: {cook_time_minutes} minutes\\n\"\n",
    "        \"Main Ingredients: {main_ingredients}\\n\"\n",
    "        \"Instructions: {instructions}\\n\"\n",
    "        \"Dietary Info: {dietary_restrictions}\"\n",
    "    )\n",
    "    context = \"\\n\\n\".join([entry_template.format(**doc) for doc in search_results])\n",
    "    prompt = prompt_strategy().format(context=context, question=question)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41c1b16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some delicious dishes you can prepare using chicken, rice, and bell pepper:\n",
      "\n",
      "### 1. **Chicken and Bell Pepper Stir-Fry**\n",
      "**Description:** A quick and vibrant stir-fry featuring tender chicken pieces, crunchy bell peppers, and flavorful spices.  \n",
      "**Cooking Instructions:**  \n",
      "1. Slice chicken breast and bell peppers into thin strips.  \n",
      "2. In a large skillet or wok, heat a tablespoon of oil over medium-high heat.  \n",
      "3. Add the chicken strips and cook until browned and cooked through (about 5-7 minutes).  \n",
      "4. Add sliced bell peppers and stir-fry for another 3-4 minutes until they are tender-crisp.  \n",
      "5. Season with soy sauce, garlic, and ginger, and cook for an additional minute.  \n",
      "6. Serve over steamed rice.  \n",
      "**Total Time:** 20 minutes  \n",
      "\n",
      "---\n",
      "\n",
      "### 2. **One-Pan Chicken Rice with Bell Peppers**\n",
      "**Description:** A hearty one-pan meal that combines chicken, bell peppers, and rice, all cooked together for an easy and tasty dinner.  \n",
      "**Cooking Instructions:**  \n",
      "1. In a large skillet, heat two tablespoons of oil over medium heat.  \n",
      "2. Season chicken with salt, pepper, and any other preferred spices, then add to the skillet. Brown on all sides (about 5-7 minutes).  \n",
      "3. Remove chicken and set aside. In the same skillet, sauté chopped onions and bell peppers until softened (about 5 minutes).  \n",
      "4. Stir in rice and cook for 1-2 minutes, then add chicken back to the pan.  \n",
      "5. Pour in broth or water, bring to a boil. Reduce heat, cover, and simmer for 20-25 minutes until rice is cooked.  \n",
      "6. Fluff with a fork and serve.  \n",
      "**Total Time:** 40 minutes  \n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Chicken Fajita Rice Bowl**\n",
      "**Description:** A flavorful bowl packed with seasoned chicken, colorful bell peppers, and fluffy rice, topped with your choice of toppings.  \n",
      "**Cooking Instructions:**  \n",
      "1. Slice chicken breast and bell peppers into strips.  \n",
      "2. In a bowl, combine olive oil, lime juice, cumin, chili powder, salt, and pepper. Marinate chicken in this mixture for at least 15 minutes.  \n",
      "3. In a skillet, cook marinated chicken over medium-high heat until cooked through (about 6-8 minutes).  \n",
      "4. Add sliced bell peppers to the skillet and cook until tender (about 3-5 minutes).  \n",
      "5. Serve the chicken and bell pepper mixture over cooked rice, and add toppings like avocado, cilantro, or cheese as desired.  \n",
      "**Total Time:** 30 minutes  \n",
      "\n",
      "---\n",
      "\n",
      "These recipes are not only easy to follow but also ensure a delicious meal that you can enjoy with your main ingredients!\n",
      "None\n",
      "Based on your ingredients—chicken, rice, and bell pepper—here are a couple of recipe suggestions along with potential substitutions for any missing ingredients.\n",
      "\n",
      "### 1. **Chicken and Bell Pepper Stir-Fry**\n",
      "#### Ingredients:\n",
      "- Chicken breast (or thighs) - can be cubed or sliced\n",
      "- Bell pepper (any color) - sliced\n",
      "- Rice (white or brown)\n",
      "- Soy sauce (or coconut aminos for a Paleo-friendly option)\n",
      "- Garlic (optional, for flavor)\n",
      "- Ginger (optional, for flavor)\n",
      "- Olive oil or sesame oil for cooking\n",
      "\n",
      "#### Instructions:\n",
      "1. Heat oil in a pan over medium-high heat.\n",
      "2. Add garlic and ginger if using, sauté until fragrant.\n",
      "3. Add chicken pieces and cook until browned and cooked through.\n",
      "4. Stir in bell peppers and cook until they are slightly tender.\n",
      "5. Add soy sauce and stir well to combine.\n",
      "6. Serve over cooked rice.\n",
      "\n",
      "#### Substitution Suggestions:\n",
      "- **Chicken**: You can use turkey, shrimp, or even tofu for a vegetarian option. This change will affect the flavor profile and texture but can work well.\n",
      "- **Rice**: Quinoa or cauliflower rice can replace traditional rice to make the dish lower-carb. Cauliflower rice will give a different texture but is excellent for a light meal.\n",
      "- **Bell Pepper**: Other vegetables like broccoli, snap peas, or carrots can be substituted. Each vegetable will alter the dish's color and crunch.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Chicken and Rice Casserole with Bell Peppers**\n",
      "#### Ingredients:\n",
      "- Chicken pieces (cut into chunks)\n",
      "- Rice\n",
      "- Sliced bell peppers (any color)\n",
      "- Cream of mushroom soup (or homemade with mushrooms, cream, and broth)\n",
      "- Onion (optional)\n",
      "- Cheese (optional, for topping)\n",
      "  \n",
      "#### Instructions:\n",
      "1. Preheat your oven to 375°F (190°C).\n",
      "2. In a large bowl, mix together the chicken, uncooked rice, sliced bell peppers, and onion if using.\n",
      "3. Pour the cream of mushroom soup over the mixture and stir to combine.\n",
      "4. Transfer to a greased baking dish and cover.\n",
      "5. Bake for about 45 minutes to an hour, until the rice is cooked and the chicken is tender.\n",
      "6. If desired, uncover in the last 10-15 minutes and sprinkle cheese on top to melt.\n",
      "\n",
      "#### Substitution Suggestions:\n",
      "- **Chicken**: Use any kind of poultry or even cooked beans for a vegetarian dish. This switch may change the cooking time if beans are used (since they need to be pre-cooked).\n",
      "- **Cream of Mushroom Soup**: If you want to make this dairy-free, you can blend soaked cashews with vegetable broth and mushrooms to create a creamy sauce.\n",
      "- **Rice**: Similar to the stir-fry, substituting with quinoa or a low-carb option like riced cauliflower will alter the texture but keep the flavors intact.\n",
      "\n",
      "### Conclusion\n",
      "Feel free to mix and match the ingredients and substitutions based on your preferences and dietary needs. These recipes are versatile, and adjusting the ingredients can lead to new and exciting flavor profiles. Enjoy your cooking!\n",
      "None\n",
      "Based on the main ingredients you’ve provided—chicken, rice, and bell pepper—here are a couple of nutritious recipe options that cater to various dietary restrictions along with health benefits and modifications:\n",
      "\n",
      "### 1. **Chicken and Bell Pepper Stir-Fry**\n",
      "\n",
      "#### Ingredients:\n",
      "- 1 lb chicken breast, sliced into thin strips\n",
      "- 1 cup bell peppers (mix of colors)\n",
      "- 2 cups cooked rice (brown or cauliflower rice for lower carbs)\n",
      "- 2 tablespoons olive oil\n",
      "- 3 cloves garlic, minced\n",
      "- 1 tablespoon soy sauce (or coconut aminos for a gluten-free option)\n",
      "- 1 teaspoon ginger, grated\n",
      "- Salt and pepper to taste\n",
      "- Optional: Green onions and sesame seeds for garnish\n",
      "\n",
      "#### Instructions:\n",
      "1. Heat the olive oil in a large skillet over medium-high heat.\n",
      "2. Add the garlic and ginger, sauté until fragrant.\n",
      "3. Add the sliced chicken and cook until browned.\n",
      "4. Toss in the bell peppers and cook until just tender.\n",
      "5. Stir in the cooked rice and soy sauce, mixing well to combine.\n",
      "6. Season with salt, pepper, and garnish with green onions and sesame seeds.\n",
      "\n",
      "#### Nutritional Benefits:\n",
      "- **High Protein**: Chicken provides a good source of lean protein, essential for muscle repair and growth.\n",
      "- **Vitamins**: Bell peppers are rich in vitamins A, C, and various antioxidants, boosting immune health.\n",
      "- **Healthy Fats**: Olive oil offers healthy monounsaturated fats beneficial for heart health.\n",
      "\n",
      "#### Modifications:\n",
      "- **Gluten-Free**: Substitute soy sauce with coconut aminos.\n",
      "- **Low-Carb/Keto**: Use cauliflower rice instead of traditional rice.\n",
      "- **Vegetarian/Vegan**: Replace chicken with tofu or tempeh and consider using nutritional yeast for added flavor.\n",
      "\n",
      "### 2. **Mediterranean Chicken and Rice Bake with Bell Peppers**\n",
      "\n",
      "#### Ingredients:\n",
      "- 1 lb chicken thighs or breasts, cubed\n",
      "- 1 cup jasmine or basmati rice\n",
      "- 1 cup bell peppers, chopped\n",
      "- 1 onion, chopped\n",
      "- 2 cups chicken broth (or vegetable broth for a vegetarian version)\n",
      "- 2 cloves garlic, minced\n",
      "- 1 teaspoon dried oregano\n",
      "- 1 teaspoon paprika\n",
      "- Salt and pepper to taste\n",
      "- Olive oil for drizzling\n",
      "\n",
      "#### Instructions:\n",
      "1. Preheat your oven to 375°F (190°C).\n",
      "2. In a baking dish, combine cubed chicken, rice, bell peppers, onion, garlic, oregano, paprika, and broth.\n",
      "3. Season with salt and pepper, and drizzle with olive oil.\n",
      "4. Cover and bake for 45-50 minutes until the chicken is cooked through and the rice is tender.\n",
      "5. Fluff with a fork and serve warm.\n",
      "\n",
      "#### Nutritional Benefits:\n",
      "- **Carbohydrates**: Rice is a good source of carbs, providing energy for your workouts and daily activities.\n",
      "- **Fiber**: Bell peppers and onion contribute dietary fiber, aiding digestion and heart health.\n",
      "- **Flavorful and Satisfying**: The dish is both flavorful and filling due to the combination of protein, carbs, and nutritious vegetables.\n",
      "\n",
      "#### Modifications:\n",
      "- **Paleo**: Replace rice with cauliflower rice and avoid the broth to keep it grain-free.\n",
      "- **Dairy-Free**: This recipe is naturally dairy-free; just ensure that no additional dairy products are included.\n",
      "- **Vegetarian/Vegan**: Substitute chicken with chickpeas or lentils and use vegetable broth for moisture.\n",
      "\n",
      "### Conclusion\n",
      "Both recipes provide a wholesome meal option featuring chicken, rice, and bell peppers, balancing protein, carbs, and essential vitamins. Depending on dietary needs, adjustments can be made to accommodate preferences while still focusing on nutrition.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test get_prompt_strategy_1, get_prompt_strategy_2 and get_prompt_strategy_3\n",
    "question = \"What can I cook with chicken, rice, and bell pepper?\"\n",
    "print(rag_minsearch(question, prompt_strategy=get_prompt_strategy_1))\n",
    "print(rag_elasticsearch(question, prompt_strategy=get_prompt_strategy_1))\n",
    "print(rag_minsearch(question, prompt_strategy=get_prompt_strategy_2))\n",
    "print(rag_elasticsearch(question, prompt_strategy=get_prompt_strategy_2))\n",
    "print(rag_minsearch(question, prompt_strategy=get_prompt_strategy_3))\n",
    "print(rag_elasticsearch(question, prompt_strategy=get_prompt_strategy_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8593ad",
   "metadata": {},
   "source": [
    "#### 8.3. Conclusion of Prompt Strategies for Minsearch and Elasticsearch\n",
    "\n",
    "All prompt strategies and both retrieval pipelines successfully generate relevant, practical recipes using chicken, rice, and bell pepper as main ingredients.\n",
    "The answers include a variety of dishes (e.g., casseroles, stir-fries, rice bowls, pilafs, stuffed peppers), showing the system’s ability to surface diverse options.\n",
    "\n",
    "The RAG pipeline, with prompt strategies, produces high-quality, context-aware, and adaptable recipe recommendations, supporting different user intents (basic cooking, substitutions, nutrition). This validates the usefulness of prompt engineering and the retrieval system for real-world recipe assistant scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b948d",
   "metadata": {},
   "source": [
    "### 9. Retrieval Evaluation Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e5af4",
   "metadata": {},
   "source": [
    "#### 9.1. Hit Rate and MRR\n",
    "\n",
    "Evaluate the final retrieval quality of the best-performing pipelines. i.e. how well the search step surfaces the ground-truth document in its top-N results. The LLM reranker (ms_best_rag_with_rerank, es_best_rag_with_rerank) produces a final answer, not a ranked list of retrieved documents, so it can't be directly used for retrieval metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e6719d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt += 1\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank]:\n",
    "                total_score += 1 / (rank + 1)\n",
    "                break\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate_retrieval(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = str(q['id'])\n",
    "        results = search_function(q)\n",
    "        relevance = [str(d.get('id')) == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09f278a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Minsearch retrieval...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221729befbb54d7eac3c2eccca825467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minsearch: {'hit_rate': 0.05, 'mrr': 0.05}\n",
      "Evaluating Elasticsearch retrieval...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8373db9c164ecd9dd46c74d8841666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearch: {'hit_rate': 0.428, 'mrr': 0.424}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Minsearch retrieval...\")\n",
    "metrics_ms = evaluate_retrieval(ground_truth, lambda q: ms_cover_then_hybrid_search(q['question'], index, num_results=10))\n",
    "print(\"Minsearch:\", metrics_ms)\n",
    "\n",
    "print(\"Evaluating Elasticsearch retrieval...\")\n",
    "metrics_es = evaluate_retrieval(ground_truth, lambda q: es_cover_then_hybrid_search(q['question'], num_results=10))\n",
    "print(\"Elasticsearch:\", metrics_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e897c5",
   "metadata": {},
   "source": [
    "#### 9.2. Conclusion of Hit Rate and MRR\n",
    "\n",
    "* Elasticsearch dramatically outperforms Minsearch on both hit rate and MRR (Mean Reciprocal Rank) for the ground-truth retrieval evaluation.\n",
    "\n",
    "    *  Hit Rate: The proportion of queries where the correct recipe appears in the top 10 results is ~36.5% for Elasticsearch, but only ~2.4% for Minsearch.\n",
    "    * MRR: The average reciprocal rank of the correct recipe is also much higher for Elasticsearch (~0.37 vs. ~0.02).\n",
    "* Minsearch is not effective for this retrieval task with the current setup and queries. It almost never surfaces the correct recipe in the top results.\n",
    "\n",
    "* Elasticsearch is much more suitable for ingredient/time-based recipe retrieval, likely due to its more advanced text search and ranking capabilities. For a production system 37% is moderate, not excellent, but may be reasonable, especially with a small dataset.\n",
    "\n",
    "Since both metrics are returning the exact same value, we can conclude that whenever the correct document is present, it is always at the same rank (almost always rank 1), or it is missing entirely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a028c95",
   "metadata": {},
   "source": [
    "### 10. Parameter Optimization for Minsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790af04",
   "metadata": {},
   "source": [
    "#### 10.1. Parameter optimization of Minsearch's text field boosts (for keyword/text search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f65448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_ranges = { # Used for optimizing text field boosts in Minsearch\n",
    "    'recipe_name': (0.0, 1.0),\n",
    "    'main_ingredients': (0.0, 4.0),\n",
    "    'all_ingredients': (0.0, 5.0),\n",
    "    'instructions': (0.0, 3.0),\n",
    "    'cuisine_type': (0.0, 1.0),\n",
    "    'dietary_restrictions': (0.0, 2.0)\n",
    "}\n",
    "\n",
    "def simple_optimize(param_ranges, objective_function, n_iterations=10):\n",
    "    best_params = None\n",
    "    best_score = float('-inf')\n",
    "    for _ in range(n_iterations):\n",
    "        current_params = {}\n",
    "        for param, (min_val, max_val) in param_ranges.items():\n",
    "            current_params[param] = random.uniform(min_val, max_val)\n",
    "        current_score = objective_function(current_params)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_params = current_params\n",
    "    return best_params, best_score\n",
    "\n",
    "gt_val = df_gt.sample(n=50, random_state=42).to_dict(orient='records')\n",
    "\n",
    "def objective(boost_params):\n",
    "    def search_function(q):\n",
    "        return index.search(q['question'], boost_dict=boost_params, num_results=10)\n",
    "    results = evaluate_retrieval(gt_val, search_function)\n",
    "    return results['mrr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c195c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5ba7208e1748e79d5e068604decc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67ca73f87634e85bbed80faee0a6218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb12ff0fdcab47b38ca31483297a98dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa8c01600a24349997db42324c948d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26eb86f8b0345c79b4d50d4a771d511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b54a10611041de9a5ac7a2e22627ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26c23511d13472780881e136aa1fab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a0739cbc384ed5b545f3feed62498a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61fffd729ab40e1bd58718f635ba883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced481de989a4a47856c098e8a239e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bae1f03f054c2bbb2b5b7ef22faa3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679467135eee47b1b5fcc7839ac30559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22b746987e841998d40fd39ea0976de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7420116fd4c4ec5a3ac6a736c568387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0e893ea0ee4ec497f37b0265e0d797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4a2fa23f2e4dacafdbf903074dfbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb3de5880924fab9e402b7a7bb16059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d450bbe9bae4a3e851dc5dee40d9d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0174c0b46f4000b730f3270f57f178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2614123f0d324f56b36b3401bed4e442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Minsearch boost params: {'recipe_name': 0.05695903438842731, 'main_ingredients': 2.918459989752664, 'all_ingredients': 1.1310668468873153, 'instructions': 0.37781160720585716, 'cuisine_type': 0.10329587286913522, 'dietary_restrictions': 0.18388683959828445}\n",
      "Best validation MRR: 0.46774603174603163\n"
     ]
    }
   ],
   "source": [
    "best_boost, best_score = simple_optimize(param_ranges, objective, n_iterations=20)\n",
    "print(\"Best Minsearch boost params:\", best_boost)\n",
    "print(\"Best validation MRR:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f06dc",
   "metadata": {},
   "source": [
    "#### 10.2. Conclusions of Parameter Optimization for Minsearch\n",
    "\n",
    "* Field importance: The optimizer found that giving a high boost to main_ingredients (3.53) and moderate boosts to all_ingredients (1.58) and instructions (0.76) improves retrieval performance. recipe_name, cuisine_type, and dietary_restrictions are less important but still contribute.\n",
    "* Retrieval quality: The best Mean Reciprocal Rank (MRR) achieved on the validation set is 0.28. This means, on average, the correct recipe appears near the top 3-4 results (since 1/0.28 ≈ 3.6).\n",
    "* Optimization effect: Tuning the field boosts can significantly improve Minsearch's retrieval effectiveness compared to default or arbitrary weights.\n",
    "\n",
    "**Conclusion of 9.2. Hit Rate and MRR for Minsearch:**\n",
    "The evaluation of 9.2. uses the default or initial Minsearch configuration (default field boosts, no optimization). Minsearch's basic keyword search is not well-tuned for the complex, ingredient/time-based queries in the ground-truth set, so it rarely surfaced the correct recipe. Still, Elasticsearch outperforms Minsearch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c703900e",
   "metadata": {},
   "source": [
    "### 11. RAG Pipeline Evaluation and LLM Answer Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa827e",
   "metadata": {},
   "source": [
    "#### 11.1. Fixed version (single prompt template)\n",
    "\n",
    "The fixed version Ensures consistency and fairness in evaluation. Every answer is generated using the same prompt style, so results are directly comparable across all questions and retrieval methods. This is important for benchmarking, metrics (like MRR/Hit Rate), and automated LLM-as-judge evaluation.\n",
    "The fixed versions are for automated, consistent evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9149730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_minsearch(question):\n",
    "    return ms_best_rag_with_rerank(question)\n",
    "\n",
    "def rag_elasticsearch(question):\n",
    "    return es_best_rag_with_rerank(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada427e",
   "metadata": {},
   "source": [
    "#### 12.2. LLM-as-Judge Evaluation (RAG answer quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c9ae942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RAG (Minsearch) with LLM-as-judge...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba42bc0f19e4c049c0868c6807095cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RAG (Elasticsearch) with LLM-as-judge...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0121ad7112634a80a995143fae447d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template_2 = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "sample = df_gt.sample(n=50, random_state=1).to_dict(orient='records')\n",
    "evaluations_ms = []\n",
    "evaluations_es = []\n",
    "\n",
    "print(\"Evaluating RAG (Minsearch) with LLM-as-judge...\")\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag_minsearch(question)\n",
    "    prompt = prompt_template_2.format(question=question, answer_llm=answer_llm)\n",
    "    evaluation = llm(prompt)\n",
    "    try:\n",
    "        evaluation = json.loads(evaluation)\n",
    "    except Exception:\n",
    "        evaluation = {\"Relevance\": \"ERROR\", \"Explanation\": evaluation}\n",
    "    evaluations_ms.append({\n",
    "        \"id\": record['id'],\n",
    "        \"question\": question,\n",
    "        \"answer\": answer_llm,\n",
    "        \"relevance\": evaluation.get(\"Relevance\"),\n",
    "        \"explanation\": evaluation.get(\"Explanation\")\n",
    "    })\n",
    "\n",
    "print(\"Evaluating RAG (Elasticsearch) with LLM-as-judge...\")\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag_elasticsearch(question)\n",
    "    prompt = prompt_template_2.format(question=question, answer_llm=answer_llm)\n",
    "    evaluation = llm(prompt)\n",
    "    try:\n",
    "        evaluation = json.loads(evaluation)\n",
    "    except Exception:\n",
    "        evaluation = {\"Relevance\": \"ERROR\", \"Explanation\": evaluation}\n",
    "    evaluations_es.append({\n",
    "        \"id\": record['id'],\n",
    "        \"question\": question,\n",
    "        \"answer\": answer_llm,\n",
    "        \"relevance\": evaluation.get(\"Relevance\"),\n",
    "        \"explanation\": evaluation.get(\"Explanation\")\n",
    "    })\n",
    "\n",
    "# Persist evaluation results for further analysis and reproducibility (record the relevance judgments)\n",
    "df_eval_ms = pd.DataFrame(evaluations_ms)\n",
    "df_eval_es = pd.DataFrame(evaluations_es)\n",
    "df_eval_ms.to_csv('../data/rag-eval-minsearch.csv', index=False)\n",
    "df_eval_es.to_csv('../data/rag-eval-elasticsearch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e849415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minsearch RAG relevance proportions:\n",
      "relevance\n",
      "RELEVANT           0.9\n",
      "PARTLY_RELEVANT    0.1\n",
      "Name: proportion, dtype: float64\n",
      "Elasticsearch RAG relevance proportions:\n",
      "relevance\n",
      "RELEVANT           0.56\n",
      "PARTLY_RELEVANT    0.42\n",
      "NON_RELEVANT       0.02\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Minsearch RAG relevance proportions:\")\n",
    "print(df_eval_ms['relevance'].value_counts(normalize=True))\n",
    "print(\"Elasticsearch RAG relevance proportions:\")\n",
    "print(df_eval_es['relevance'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd240bb",
   "metadata": {},
   "source": [
    "#### 12.3. Conclusion of LLM-as-Judge Evaluation\n",
    "\n",
    "* Minsearch RAG pipeline produces highly relevant answers:\n",
    "\n",
    "    * 94% of answers are judged RELEVANT by the LLM-as-judge.\n",
    "    * Only 4% are PARTLY_RELEVANT and 2% NON_RELEVANT.\n",
    "    * This indicates Minsearch is very effective at generating fully relevant answers for the evaluated questions.\n",
    "\n",
    "* Elasticsearch RAG pipeline is also strong, but less so than Minsearch in this evaluation:\n",
    "\n",
    "    * 72% of answers are RELEVANT.\n",
    "    * 22% are PARTLY_RELEVANT and 6% NON_RELEVANT.\n",
    "    * This suggests Elasticsearch sometimes produces answers that are only partially relevant or not relevant.\n",
    "\n",
    "* Both systems perform well, but Minsearch achieves a higher proportion of fully relevant answers in this test.\n",
    "* Minsearch may be better for maximizing answer relevance on this dataset and evaluation set, while Elasticsearch is still strong but with more partially or non-relevant outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8657bd4",
   "metadata": {},
   "source": [
    "### 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40397505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RETRIEVAL METRICS ===\n",
      "Minsearch: {'hit_rate': 0.05, 'mrr': 0.05}\n",
      "Elasticsearch: {'hit_rate': 0.428, 'mrr': 0.424}\n",
      "Best Minsearch boost params: {'recipe_name': 0.05695903438842731, 'main_ingredients': 2.918459989752664, 'all_ingredients': 1.1310668468873153, 'instructions': 0.37781160720585716, 'cuisine_type': 0.10329587286913522, 'dietary_restrictions': 0.18388683959828445}\n",
      "Best validation MRR: 0.46774603174603163\n",
      "\n",
      "=== RAG LLM-as-Judge (proportion RELEVANT) ===\n",
      "Minsearch: 0.9\n",
      "Elasticsearch: 0.56\n",
      "\n",
      "All evaluation results saved to CSV in ../data/\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== RETRIEVAL METRICS ===\")\n",
    "print(\"Minsearch:\", metrics_ms)\n",
    "print(\"Elasticsearch:\", metrics_es)\n",
    "print(\"Best Minsearch boost params:\", best_boost)\n",
    "print(\"Best validation MRR:\", best_score)\n",
    "print(\"\\n=== RAG LLM-as-Judge (proportion RELEVANT) ===\")\n",
    "print(\"Minsearch:\", (df_eval_ms['relevance'] == 'RELEVANT').mean())\n",
    "print(\"Elasticsearch:\", (df_eval_es['relevance'] == 'RELEVANT').mean())\n",
    "print(\"\\nAll evaluation results saved to CSV in ../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981e6512",
   "metadata": {},
   "source": [
    "Stop Elasticsearch running in Docker:\n",
    "\n",
    "`docker stop elasticsearch`\n",
    "\n",
    "Remove the container (optional, if you want to delete it):\n",
    "\n",
    "`docker rm elasticsearch`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipe-assistant-P5TWTFE5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
