{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1919fb56",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) for Recipe Assistant: RAG Evaluation\n",
    "\n",
    "This notebook provides an evaluation framework for the RAG system described in `rag-flow.ipynb`.  \n",
    "It covers ground-truth generation, retrieval metrics (Hit Rate, MRR), parameter optimization, and LLM-as-judge answer quality for both Minsearch and Elasticsearch using the best-performing combined retrieval and reranking approaches.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db931079",
   "metadata": {},
   "source": [
    "### 1. Set-up Dependencies, OpenAI Client and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d6aa1",
   "metadata": {},
   "source": [
    "#### 1.1 Dependencies and OpenAI Client\n",
    "Import dependencies, load OpenAI API key and connect to OpenAI API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91136bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import dotenv\n",
    "import minsearch\n",
    "from elasticsearch import Elasticsearch\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a93e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(\"../.env\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d5858",
   "metadata": {},
   "source": [
    "#### 1.2. Load and Index Recipe Data\n",
    "Load the recipe dataset from CSV file and prepare it for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad929e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/recipes_clean.csv')\n",
    "\n",
    "# Add an ID column if it doesn't exist\n",
    "if 'id' not in df.columns:\n",
    "    df['id'] = range(len(df))\n",
    "    \n",
    "# Create documents for indexing\n",
    "documents = df.to_dict(orient='records')\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fa8542",
   "metadata": {},
   "source": [
    "### 2. Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a98abf",
   "metadata": {},
   "source": [
    "#### 2.1 Create Time Filter\n",
    "Users will be queried for available time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e736acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_max_time(results, max_time=None):\n",
    "    if max_time is None:\n",
    "        return results\n",
    "    filtered = []\n",
    "    for doc in results:\n",
    "        try:\n",
    "            total_time = int(doc.get('prep_time_minutes', 0)) + int(doc.get('cook_time_minutes', 0))\n",
    "        except Exception:\n",
    "            total_time = 99999\n",
    "        if total_time <= max_time:\n",
    "            filtered.append(doc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1163bf3b",
   "metadata": {},
   "source": [
    "#### 2.2 Print Unused Ingredients\n",
    "\n",
    "Utility function that, given the query and the results, prints which query ingredients were not covered by the selected recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fdc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize_ingredients is used in below Cover Ingredients Searches\n",
    "def tokenize_ingredients(ingredient_str):\n",
    "    # Split only on commas, strip whitespace, and lowercase\n",
    "    return set([ing.strip().lower() for ing in ingredient_str.split(',') if ing.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e230466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unused_ingredients(ingredients, results):\n",
    "    query_ings = tokenize_ingredients(ingredients)\n",
    "    used = set()\n",
    "    for doc in results:\n",
    "        recipe_ings = tokenize_ingredients(doc.get('all_ingredients', ''))\n",
    "        used |= recipe_ings\n",
    "    unused = query_ings - used\n",
    "    print(\"Unused ingredients:\", \", \".join(unused) if unused else \"All ingredients used!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66fd2b1",
   "metadata": {},
   "source": [
    "#### 2.3. Deduplicate Search results\n",
    "Elasticsearch and LLMs can sometimes return duplicate or near-duplicate documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe61132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_results(results):\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for doc in results:\n",
    "        # Use a unique field, e.g. 'id' or a tuple of fields\n",
    "        key = doc.get('id') or (doc.get('recipe_name'), doc.get('prep_time_minutes'), doc.get('cook_time_minutes'))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(doc)\n",
    "    return deduped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f061a",
   "metadata": {},
   "source": [
    "### 3. Ground Truth Generation\n",
    "Generate ground-truth user questions for each recipe using the LLM.  \n",
    "This is used to evaluate retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26917b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> The fields are the most relevant for generating realistic user questions, given the prompt instructions.\n",
    "prompt_template = \"\"\"\n",
    "You emulate a user of our recipe assistant application.\n",
    "Formulate 5 questions this user might ask based on a provided recipe.\n",
    "Make the questions specific to ingredients, cooking methods, \n",
    "cooking duration (prep/cook time), or dietary information in this recipe.\n",
    "Do NOT mention the recipe name in the question.\n",
    "The record should contain the answer to the questions, \n",
    "and the questions should be complete and not too short.\n",
    "Use as few words as possible from the record.\n",
    "\n",
    "The record:\n",
    "\n",
    "Recipe: {recipe_name}\n",
    "Cuisine: {cuisine_type}\n",
    "Main Ingredients: {main_ingredients}\n",
    "Instructions: {instructions}\n",
    "Dietary Info: {dietary_restrictions}\n",
    "Total Time: {total_time_minutes}\n",
    "Servings: {servings}\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks:\n",
    "\n",
    "{{\"questions\": [\"question1\", \"question2\", ..., \"question5\"]}}\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_questions(doc):\n",
    "    prompt = prompt_template.format(**doc)\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "results = {}\n",
    "for i, doc in enumerate(tqdm(documents[:100])):\n",
    "    doc_id = doc.get('id', i)\n",
    "    if doc_id in results:\n",
    "        continue\n",
    "    try:\n",
    "        questions_raw = generate_questions(doc)\n",
    "        questions = json.loads(questions_raw)\n",
    "        results[doc_id] = questions['questions']\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        continue\n",
    "\n",
    "final_results = []\n",
    "for doc_id, questions in results.items():\n",
    "    for q in questions:\n",
    "        final_results.append((doc_id, q))\n",
    "\n",
    "df_results = pd.DataFrame(final_results, columns=['id', 'question'])\n",
    "df_results.to_csv('../data/ground-truth-retrieval.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e791b5a6",
   "metadata": {},
   "source": [
    "### 4. Load Ground Truth Questions\n",
    "\n",
    "Load the generated ground-truth questions for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8eb9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt = pd.read_csv('../data/ground-truth-retrieval.csv')\n",
    "ground_truth = df_gt.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24269e97",
   "metadata": {},
   "source": [
    "### 5. Minsearch Setup and Retrieval Strategies\n",
    "Set up Minsearch retrieval backend as in the main RAG notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448c394",
   "metadata": {},
   "source": [
    "#### 5.1. Minsearch Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep/cook time and servings are numeric, and not suitable for Minsearch's default text/keyword search.\n",
    "# Setup minsearch index for recipes\n",
    "index = minsearch.Index(\n",
    "    text_fields=['recipe_name', 'main_ingredients', 'all_ingredients', \\\n",
    "                 'instructions', 'cuisine_type', 'dietary_restrictions'],\n",
    "    keyword_fields=['meal_type', 'difficulty_level']\n",
    ")\n",
    "\n",
    "# Fit/train the index on the recipe documents\n",
    "index.fit(documents)\n",
    "index.documents = index.docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01739d",
   "metadata": {},
   "source": [
    "#### 5.2 Run RAG Retrieval Strategies for Evaluation (Minsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409c587",
   "metadata": {},
   "source": [
    "##### Approach 4: Hybrid Search With OpenAI (Keyword + Embedding)\n",
    "Combines keyword and with OpenAI's embedding-based (semantic) similarity for more robust retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab89928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    response = client.embeddings.create( # OpenAI client\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=[text]\n",
    "    )\n",
    "    return np.array(response.data[0].embedding)\n",
    "\n",
    "# Precompute embeddings for all recipes (DO THIS ONCE AND CACHE)\n",
    "if not hasattr(index, \"embeddings\"):\n",
    "    index.embeddings = [get_embedding(doc['all_ingredients']) for doc in index.documents]\n",
    "\n",
    "def ms_hybrid_search(query, index, num_results=5, alpha=0.5, max_time=None):\n",
    "    # Keyword search using SimpleIndex\n",
    "    keyword_results = index.search(query, num_results=10)\n",
    "    # Embedding search\n",
    "    query_emb = get_embedding(query)\n",
    "    similarities = [np.dot(query_emb, emb) for emb in index.embeddings]\n",
    "    top_indices = np.argsort(similarities)[-10:][::-1]\n",
    "    embedding_results = [index.documents[i] for i in top_indices]\n",
    "    # Combine results (simple union, or weighted score)\n",
    "    combined = {}\n",
    "    for doc in keyword_results:\n",
    "        combined[doc['id']] = alpha\n",
    "    for doc in embedding_results:\n",
    "        combined[doc['id']] = combined.get(doc['id'], 0) + (1 - alpha)\n",
    "    # Sort by combined score\n",
    "    sorted_ids = sorted(combined, key=combined.get, reverse=True)\n",
    "    results = [doc for doc in index.documents if doc['id'] in sorted_ids]\n",
    "    results = filter_by_max_time(results, max_time)\n",
    "    # Deduplicate before returning (optional but recommended if you ever combine sources)\n",
    "    deduped_results = deduplicate_results(results)\n",
    "    return deduped_results[:num_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c04f2eb",
   "metadata": {},
   "source": [
    "##### Approach 5: Cover Ingredients Search\n",
    "Selects a set of recipes that together cover as many of the query ingredients as possible, optionally filtering out recipes that exceed the max_time constraint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Already run above ***\n",
    "def tokenize_ingredients(ingredient_str):\n",
    "    # Split only on commas, strip whitespace, and lowercase\n",
    "    return set([ing.strip().lower() for ing in ingredient_str.split(',') if ing.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_cover_ingredients_search(query, index, num_results=5, max_time=None):\n",
    "    \"\"\"\n",
    "    Selects a set of recipes that together cover as many of the query ingredients as possible,\n",
    "    optionally filtering out recipes that exceed the max_time constraint.\n",
    "    \"\"\"\n",
    "    query_ings = tokenize_ingredients(query)\n",
    "    uncovered = set(query_ings)\n",
    "    selected = []\n",
    "    docs = index.documents.copy() \n",
    "\n",
    "    # Apply time filter to docs at the start for efficiency\n",
    "    docs = filter_by_max_time(docs, max_time)\n",
    "\n",
    "    while uncovered and len(selected) < num_results and docs:\n",
    "        best_doc = None\n",
    "        best_overlap = 0\n",
    "        for doc in docs:\n",
    "            recipe_ings = tokenize_ingredients(doc.get('all_ingredients', ''))\n",
    "            overlap = len(uncovered & recipe_ings)\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_doc = doc\n",
    "        if best_doc and best_overlap > 0:\n",
    "            selected.append(best_doc)\n",
    "            recipe_ings = tokenize_ingredients(best_doc.get('all_ingredients', ''))\n",
    "            uncovered -= recipe_ings\n",
    "            docs.remove(best_doc)\n",
    "        else:\n",
    "            break\n",
    "    # Deduplicate results before returning (recommended if docs may have overlap)\n",
    "    deduped_results = deduplicate_results(selected)\n",
    "    return deduped_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307cb4b",
   "metadata": {},
   "source": [
    "#### 5.3. Build Prompt for the LLM Using Retrieved Context (Minsearch + Elasticsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd44478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    entry_template = \"\"\"\n",
    "Recipe: {recipe_name}\n",
    "Cuisine: {cuisine_type}\n",
    "Meal Type: {meal_type}\n",
    "Difficulty: {difficulty_level}\n",
    "Prep Time: {prep_time_minutes} minutes\n",
    "Cook Time: {cook_time_minutes} minutes\n",
    "Main Ingredients: {main_ingredients}\n",
    "Instructions: {instructions}\n",
    "Dietary Info: {dietary_restrictions}\n",
    "\"\"\".strip()\n",
    "    context = \"\\n\\n\".join([entry_template.format(**doc) for doc in search_results])\n",
    "    prompt_template = \"\"\"\n",
    "You are an expert chef and culinary assistant. Answer the question based on the content from our recipe database.\n",
    "Use only the facts from the context when answering the question.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide recipe recommendations with brief explanations of why they match the requested ingredients.\n",
    "If exact ingredients aren't available, suggest the closest matches and mention any substitutions needed.\n",
    "\"\"\".strip()\n",
    "    return prompt_template.format(context=context, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d07e17",
   "metadata": {},
   "source": [
    "#### 5.4 LLM Call Function (Minsearch + Elasticsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1df80c",
   "metadata": {},
   "source": [
    "#### 5.5 RAG Pipeline Using Minsearch\n",
    "Test ms_cover_ingredients_search and ms_hybrid_search on the RAG Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO I NEED THIS FOR EVALUATING ms_cover_then_hybrid_search?\n",
    "def rag_minsearch(query, max_time=None, num_results=5, approach=\"cover\"): \n",
    "    \"\"\"\n",
    "    approach: \"cover\" for ms_cover_ingredients_search, \"hybrid\" for ms_hybrid_search\n",
    "    Deduplicates results before building prompt.\n",
    "    \"\"\"\n",
    "    if approach == \"cover\":\n",
    "        search_results = ms_cover_ingredients_search(query, index=index, num_results=num_results, max_time=max_time)\n",
    "        deduped_results = deduplicate_results(search_results)\n",
    "    elif approach == \"hybrid\":\n",
    "        search_results = ms_hybrid_search(query, index=index, num_results=num_results, max_time=max_time)\n",
    "        deduped_results = deduplicate_results(search_results)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown approach: choose 'cover' or 'hybrid'\")\n",
    "    prompt = build_prompt(query, deduped_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc586420",
   "metadata": {},
   "source": [
    "#### 5.6 Combine Cover and Hybrid RAG Pipelines (Minsearch)\n",
    "\n",
    "Combine the two approaches by first running Cover Ingredients Search to select a diverse set of recipes that cover as many ingredients as possible, and then passing those results through the Hybrid Search to rerank or filter them by semantic relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2667098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_cover_then_hybrid_search(query, index, num_results=5, max_time=None, hybrid_top_k=5):\n",
    "    \"\"\"\n",
    "    Combine Cover Ingredients Search and Hybrid Search:\n",
    "    1. Run Cover Ingredients Search to get a diverse set of recipes (pool size = num_results*4).\n",
    "    2. Rerank the pool by semantic similarity to the query using OpenAI embeddings.\n",
    "    3. Deduplicate results before returning.\n",
    "    \"\"\"\n",
    "    # Step 1: Cover Ingredients Search to get a diverse set and increase the pool size\n",
    "    cover_results = ms_cover_ingredients_search(query, index, num_results=num_results*4, max_time=max_time)\n",
    "    if not cover_results:\n",
    "        return []\n",
    "    # Step 2: Hybrid Search, but restrict to cover_results as the candidate pool\n",
    "    # Compute embedding for the query\n",
    "    query_emb = get_embedding(query)\n",
    "    # Compute similarities only for cover_results\n",
    "    cover_embeddings = [get_embedding(doc['all_ingredients']) for doc in cover_results]\n",
    "    similarities = [np.dot(query_emb, emb) for emb in cover_embeddings]\n",
    "    # Get top-k by semantic similarity\n",
    "    top_indices = np.argsort(similarities)[-hybrid_top_k:][::-1]\n",
    "    # The hybrid logic is built into the similarity ranking (symbolic/keyword + neural/embedding/semantic)\n",
    "    hybrid_results = [cover_results[i] for i in top_indices] \n",
    "    # Deduplicate before returning (defensive, in case of overlap)\n",
    "    deduped_results = deduplicate_results(hybrid_results[:num_results])\n",
    "    return deduped_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d752ee",
   "metadata": {},
   "source": [
    "### 6. Elasticsearch Setup and Retrieval Strategies\n",
    "Set up Elasticsearch retrieval backend as in the main RAG notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05928346",
   "metadata": {},
   "source": [
    "#### 6.1 Elasticsearch Docker, Client and Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6befe638",
   "metadata": {},
   "source": [
    "---\n",
    "Run docker in terminal:\n",
    "\n",
    "`docker run -d --name elasticsearch -p 9200:9200 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:8.13.4`\n",
    "\n",
    "Or:\n",
    "\n",
    "`docker run -d --name elasticsearch \\\n",
    "  -p 9200:9200 \\\n",
    "  -e \"discovery.type=single-node\" \\\n",
    "  -e \"xpack.security.enabled=false\" \\\n",
    "  -e \"ES_JAVA_OPTS=-Xms512m -Xmx1g\" \\\n",
    "  docker.elastic.co/elasticsearch/elasticsearch:8.13.4`\n",
    "\n",
    "And check if Elasticsearch is up:\n",
    "\n",
    "`curl http://localhost:9200`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create Elasticsearch Client (make sure Elasticsearch is running locally)\n",
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b513183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Elasticsearch index settings and mappings for recipes\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1, # A unit of storage and search. More shards can improve parallelism for large datasets\n",
    "        \"number_of_replicas\": 0 # A shard copy for fault tolerance and increased search throughput (not recommended for production)\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"recipe_name\": {\"type\": \"text\"},\n",
    "            \"main_ingredients\": {\"type\": \"text\"},\n",
    "            \"all_ingredients\": {\"type\": \"text\"},\n",
    "            \"instructions\": {\"type\": \"text\"},\n",
    "            \"cuisine_type\": {\"type\": \"text\"},\n",
    "            \"dietary_restrictions\": {\"type\": \"text\"},\n",
    "            \"meal_type\": {\"type\": \"keyword\"},\n",
    "            \"difficulty_level\": {\"type\": \"keyword\"},\n",
    "            \"prep_time_minutes\": {\"type\": \"integer\"},\n",
    "            \"cook_time_minutes\": {\"type\": \"integer\"},\n",
    "            \"all_ingredients_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 1536  # text-embedding-3-small returns 1536-dimensional vectors\n",
    "}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"recipes\"\n",
    "\n",
    "# Create the index (ignore error if it already exists)\n",
    "try:\n",
    "    es_client.indices.create(index=index_name, body=index_settings)\n",
    "except Exception as e:\n",
    "    print(\"Index may already exist:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b7bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cadc8b5ce0442eae7ff5e97801a4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/477 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Index documents into Elasticsearch, including the embedding vector\n",
    "for doc in tqdm(documents):\n",
    "    doc['all_ingredients_vector'] = get_embedding(doc['all_ingredients']).tolist()\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c905e4",
   "metadata": {},
   "source": [
    "#### 6.2 Run RAG Retrieval Strategies for Evaluation (Elasticsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5e53a",
   "metadata": {},
   "source": [
    "##### Approach 1: Basic Keyword Search\n",
    "\n",
    "Returns recipes that match the query using simple keyword matching across all fields. **Will be used in cover strategy simulation for Elasticsearch**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e108a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Search\n",
    "def es_basic_search(query, num_results=5, max_time=None):\n",
    "    search_query = {\n",
    "        \"size\": num_results * 2,  # get more to allow filtering\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [ # Elasticsearch's multi_match query is for text searching\n",
    "                    \"recipe_name\",\n",
    "                    \"main_ingredients^2\",\n",
    "                    \"all_ingredients^3\",\n",
    "                    \"instructions^1.5\",\n",
    "                    \"cuisine_type\",\n",
    "                    \"dietary_restrictions^1.5\"\n",
    "                ],\n",
    "                \"type\": \"best_fields\" # default, can be \"most_fields\", \"cross_fields\", etc.\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    result_docs = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    # Apply time filter\n",
    "    if max_time is not None:\n",
    "        result_docs = filter_by_max_time(result_docs, max_time)\n",
    "    return result_docs[:num_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc770b4a",
   "metadata": {},
   "source": [
    "##### Approach 4: Hybrid Search With OpenAI (Keyword + Embedding)\n",
    "Combines keyword and with OpenAI's embedding-based (semantic) similarity for more robust retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b28fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search (keyword + vector, requires Elasticsearch vector plugin and precomputed vectors)\n",
    "def es_hybrid_search(query, num_results=5, max_time=None):\n",
    "    # This assumes you have a vector field called 'all_ingredients_vector' in your index\n",
    "    # and a function get_embedding(text) that returns a vector for the query.\n",
    "    query_vector = get_embedding(query).tolist()\n",
    "    search_query = {\n",
    "        \"size\": num_results * 2,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\n",
    "                            \"main_ingredients^2\",\n",
    "                            \"all_ingredients^3\",\n",
    "                            \"instructions^1.5\",\n",
    "                            \"cuisine_type\",\n",
    "                            \"dietary_restrictions^1.5\",\n",
    "                            \"recipe_name\"\n",
    "                        ],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'all_ingredients_vector') + 1.0\",\n",
    "                    \"params\": {\"query_vector\": query_vector},\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    result_docs = [hit['_source'] for hit in response['hits']['hits']]\n",
    "    if max_time is not None:\n",
    "        result_docs = filter_by_max_time(result_docs, max_time)\n",
    "    # Deduplicate before returning (recommended for hybrid search)\n",
    "    deduped_results = deduplicate_results(result_docs)\n",
    "    return deduped_results[:num_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97289f38",
   "metadata": {},
   "source": [
    "##### Approach 5: Cover Ingredients Search\n",
    "Elasticsearch queries return a ranked list based on scoring, but do not natively support iterative, set-cover-style selection across multiple results. A workaround is to retrieve many candidates from Elasticsearch, then run the cover algorithm in Python on those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d8d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_cover_ingredients_search(query, num_results=5, max_time=None, candidate_pool_size=200):\n",
    "    \"\"\"\n",
    "    Simulate Cover Ingredients Search using Elasticsearch results, with optional time filtering.\n",
    "    Returns a set of recipes that together cover as many of the query ingredients as possible.\n",
    "    Deduplicates results before returning.\n",
    "    \"\"\"\n",
    "    # Step 1: Get a large pool of candidates from ES (basic search, large pool)\n",
    "    candidates = es_basic_search(query, num_results=candidate_pool_size)\n",
    "    # Step 2: Filter by time if needed\n",
    "    if max_time is not None:\n",
    "        candidates = filter_by_max_time(candidates, max_time)\n",
    "    # Step 3: Apply greedy cover algorithm\n",
    "    query_tokens = set(re.sub(r'[^\\w\\s]', '', query.lower()).replace(',', ' ').split())\n",
    "    uncovered = set(query_tokens)\n",
    "    selected = []\n",
    "    docs = candidates.copy()\n",
    "    while uncovered and len(selected) < num_results and docs:\n",
    "        best_doc = None\n",
    "        best_overlap = 0\n",
    "        for doc in docs:\n",
    "            ingredients = set(re.sub(r'[^\\w\\s]', '', str(doc.get('all_ingredients', '')).lower()).replace(',', ' ').split())\n",
    "            overlap = len(uncovered & ingredients)\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_doc = doc\n",
    "        if best_doc and best_overlap > 0:\n",
    "            selected.append(best_doc)\n",
    "            ingredients = set(re.sub(r'[^\\w\\s]', '', str(best_doc.get('all_ingredients', '')).lower()).replace(',', ' ').split())\n",
    "            uncovered -= ingredients\n",
    "            docs.remove(best_doc)\n",
    "        else:\n",
    "            break\n",
    "    # Deduplicate results before returning (recommended if docs may have overlap)\n",
    "    deduped_results = deduplicate_results(selected)\n",
    "    return deduped_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cf9b0",
   "metadata": {},
   "source": [
    "#### 6.3 RAG Pipeline Using Elasticsearch \n",
    "Test es_cover_ingredients_search and es_hybrid_search on the RAG Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO I NEED THIS FOR EVALUATING ms_cover_then_hybrid_search?\n",
    "def rag_elasticsearch(query, max_time=None, num_results=5, approach=\"cover\"):  \n",
    "    \"\"\"\n",
    "    approach: \"cover\" for es_cover_ingredients_search, \"hybrid\" for es_hybrid_search\n",
    "    \"\"\"\n",
    "    if approach == \"cover\":\n",
    "        search_results = es_cover_ingredients_search(query, num_results=num_results, max_time=max_time)\n",
    "        deduped_results = deduplicate_results(search_results)\n",
    "    elif approach == \"hybrid\":\n",
    "        search_results = es_hybrid_search(query, num_results=num_results, max_time=max_time)\n",
    "        deduped_results = deduplicate_results(search_results)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown approach: choose 'cover' or 'hybrid'\")\n",
    "    prompt = build_prompt(query, deduped_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e19f8",
   "metadata": {},
   "source": [
    "#### 6.4 Combine Cover and Hybrid RAG Pipelines (Elasticsearch)\n",
    "\n",
    "Combine the two approaches by first running Cover Ingredients Search to select a diverse set of recipes that cover as many ingredients as possible, and then passing those results through the Hybrid Search to rerank or filter them by semantic relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_cover_then_hybrid_search(query, num_results=5, max_time=None, hybrid_top_k=5, candidate_pool_size=200):\n",
    "    \"\"\"\n",
    "    Combine Cover Ingredients Search and Hybrid Search for Elasticsearch:\n",
    "    1. Run Cover Ingredients Search to get a diverse set of recipes (pool size = candidate_pool_size).\n",
    "    2. Rerank the pool by semantic similarity to the query using OpenAI embeddings.\n",
    "    3. Remove 'all_ingredients_vector' from each result for cleaner output.\n",
    "    4. Deduplicate results before returning.\n",
    "    \"\"\"\n",
    "    # Step 1: Cover Ingredients Search to get a diverse set and increase the pool size\n",
    "    cover_results = es_cover_ingredients_search(query, num_results=candidate_pool_size, max_time=max_time)\n",
    "    if not cover_results:\n",
    "        return []\n",
    "    # Step 2: Hybrid Search, but restrict to cover_results as the candidate pool\n",
    "    query_emb = get_embedding(query)\n",
    "    cover_embeddings = [get_embedding(doc['all_ingredients']) for doc in cover_results]\n",
    "    similarities = [np.dot(query_emb, emb) for emb in cover_embeddings]\n",
    "    top_indices = np.argsort(similarities)[-hybrid_top_k:][::-1]\n",
    "    # The hybrid logic is built into the similarity ranking (symbolic/keyword + neural/embedding/semantic)\n",
    "    hybrid_results = [cover_results[i] for i in top_indices] \n",
    "    # Remove all_ingredients_vector from each result\n",
    "    for doc in hybrid_results:\n",
    "        doc.pop('all_ingredients_vector', None)\n",
    "\n",
    "    return hybrid_results[:num_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca3585",
   "metadata": {},
   "source": [
    "### 7. Rerank with LLM\n",
    "\n",
    "Re-ranking means taking an initial set of retrieved documents (candidates) and re-ordering them, often using a more sophisticated model (like an LLM), to improve the relevance of the top results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3162012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_llm(query, candidates, max_time=None): \n",
    "    # Apply time filter before reranking\n",
    "    if max_time is not None:\n",
    "        candidates = filter_by_max_time(candidates, max_time)\n",
    "    context = \"\\n\\n\".join([f\"Recipe: {doc['recipe_name']}\\nIngredients: {doc['main_ingredients']}\" for doc in candidates])\n",
    "    prompt = f\"\"\"\n",
    "Given the following user query and candidate recipes, rank the recipes from most to least relevant.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Candidates:\n",
    "{context}\n",
    "\n",
    "Return a JSON list of recipe names in ranked order.\n",
    "\"\"\".strip()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    import re\n",
    "    content = response.choices[0].message.content\n",
    "    json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
    "    if json_match:\n",
    "        ranked_names = json.loads(json_match.group())\n",
    "    else:\n",
    "        return candidates\n",
    "    ranked_docs = [doc for name in ranked_names for doc in candidates if doc['recipe_name'] == name]\n",
    "    return ranked_docs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b563c",
   "metadata": {},
   "source": [
    "#### 7.1 Re-rank the ms_cover_then_hybrid_search RAG Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define a query and get some candidate recipes (using any retrieval function)\n",
    "query = \"tomato, flour, sugar, chicken, rice, butter, chocolate, shrimp, potatoes, eggs, milk, pasta, cheese, garlic, onion, bell pepper, fish, lemon, thyme\"\n",
    "candidates = ms_cover_then_hybrid_search(query, index, num_results=5)\n",
    "\n",
    "# 2. Rerank the candidates using the LLM\n",
    "reranked = rerank_with_llm(query, candidates)\n",
    "\n",
    "# 3. Print the reranked recipe names\n",
    "for doc in reranked:\n",
    "    print(doc['recipe_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae80298",
   "metadata": {},
   "source": [
    "#### 7.2. Re-rank the es_cover_then_hybrid_search RAG Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ee268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define a query and get some candidate recipes (using any retrieval function)\n",
    "query = \"tomato, flour, sugar, chicken, rice, butter, chocolate, shrimp, potatoes, eggs, milk, pasta, cheese, garlic, onion, bell pepper, fish, lemon, thyme\"\n",
    "candidates = es_cover_then_hybrid_search(query, num_results=5)\n",
    "\n",
    "# 2. Rerank the candidates using the LLM\n",
    "reranked = rerank_with_llm(query, candidates)\n",
    "\n",
    "# 3. Print the reranked recipe names\n",
    "for doc in reranked:\n",
    "    print(doc['recipe_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c04b39",
   "metadata": {},
   "source": [
    "#### 7.1 Run and Compare Both Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927791e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = \"tomato, flour, sugar, chicken, rice, butter, chocolate, shrimp, potatoes, eggs, milk, pasta, cheese, garlic, onion, bell pepper, fish, lemon, thyme\"\n",
    "max_time = 45\n",
    "\n",
    "print(\"Minsearch Cover + Hybrid Search (before rerank):\")\n",
    "ms_candidates = ms_cover_then_hybrid_search(ingredients, index, num_results=5, max_time=max_time)\n",
    "for doc in ms_candidates:\n",
    "    print(doc['recipe_name'])\n",
    "print_unused_ingredients(ingredients, ms_candidates)\n",
    "\n",
    "print(\"\\nMinsearch Cover + Hybrid Search (after rerank):\")\n",
    "ms_reranked = rerank_with_llm(ingredients, ms_candidates, max_time=max_time)\n",
    "for doc in ms_reranked:\n",
    "    print(doc['recipe_name'])\n",
    "print_unused_ingredients(ingredients, ms_reranked)\n",
    "\n",
    "print(\"\\nElasticsearch Cover + Hybrid Search (before rerank):\")\n",
    "es_candidates = es_cover_then_hybrid_search(ingredients, num_results=5, max_time=max_time)\n",
    "for doc in es_candidates:\n",
    "    print(doc['recipe_name'])\n",
    "print_unused_ingredients(ingredients, es_candidates)\n",
    "\n",
    "print(\"\\nElasticsearch Cover + Hybrid Search (after rerank):\")\n",
    "es_reranked = rerank_with_llm(ingredients, es_candidates, max_time=max_time)\n",
    "for doc in es_reranked:\n",
    "    print(doc['recipe_name'])\n",
    "print_unused_ingredients(ingredients, es_reranked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e3752",
   "metadata": {},
   "source": [
    "### 8. Prompt Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_strategy_1():\n",
    "    return \"\"\"\n",
    "You are a chef assistant. Based on the available recipes, recommend dishes that use the requested ingredients.\n",
    "Provide the recipe name, brief description, and cooking instructions and time.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "def get_prompt_strategy_2():\n",
    "    return \"\"\"\n",
    "You are an expert chef specializing in ingredient substitutions. When users provide ingredients,\n",
    "recommend recipes and suggest alternatives for missing ingredients. Always explain possible substitutions\n",
    "and how they might affect the dish.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide recommendations with substitution suggestions:\n",
    "\"\"\".strip()\n",
    "\n",
    "def get_prompt_strategy_3():\n",
    "    return \"\"\"\n",
    "You are a nutritionist and chef. Recommend recipes based on ingredients provided, considering\n",
    "nutritional value and dietary restrictions. Highlight health benefits and suggest modifications\n",
    "for different dietary needs (vegetarian, gluten-free, etc.).\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide nutritionally-aware recommendations:\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b948d",
   "metadata": {},
   "source": [
    "### 10. Retrieval Evaluation: Hit Rate and MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6719d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt += 1\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank]:\n",
    "                total_score += 1 / (rank + 1)\n",
    "                break\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate_retrieval(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = str(q['id'])\n",
    "        results = search_function(q)\n",
    "        relevance = [str(d.get('id')) == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }\n",
    "\n",
    "print(\"Evaluating Minsearch retrieval...\")\n",
    "metrics_minsearch = evaluate_retrieval(ground_truth, lambda q: ms_cover_then_hybrid_search(q['question'], index, num_results=10))\n",
    "print(\"Minsearch:\", metrics_minsearch)\n",
    "\n",
    "print(\"Evaluating Elasticsearch retrieval...\")\n",
    "metrics_es = evaluate_retrieval(ground_truth, lambda q: es_cover_then_hybrid_search(q['question'], num_results=10))\n",
    "print(\"Elasticsearch:\", metrics_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a028c95",
   "metadata": {},
   "source": [
    "### 11. Parameter Optimization for Minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f65448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_ranges = { # Used for optimizing text field boosts in Minsearch\n",
    "    'recipe_name': (0.0, 1.0),\n",
    "    'main_ingredients': (0.0, 4.0),\n",
    "    'all_ingredients': (0.0, 5.0),\n",
    "    'instructions': (0.0, 3.0),\n",
    "    'cuisine_type': (0.0, 1.0),\n",
    "    'dietary_restrictions': (0.0, 2.0)\n",
    "}\n",
    "\n",
    "def simple_optimize(param_ranges, objective_function, n_iterations=10):\n",
    "    best_params = None\n",
    "    best_score = float('-inf')\n",
    "    for _ in range(n_iterations):\n",
    "        current_params = {}\n",
    "        for param, (min_val, max_val) in param_ranges.items():\n",
    "            current_params[param] = random.uniform(min_val, max_val)\n",
    "        current_score = objective_function(current_params)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_params = current_params\n",
    "    return best_params, best_score\n",
    "\n",
    "gt_val = df_gt.sample(n=50, random_state=42).to_dict(orient='records')\n",
    "\n",
    "def objective(boost_params):\n",
    "    def search_function(q):\n",
    "        return index.search(q['question'], boost_dict=boost_params, num_results=10)\n",
    "    results = evaluate_retrieval(gt_val, search_function)\n",
    "    return results['mrr']\n",
    "\n",
    "best_boost, best_score = simple_optimize(param_ranges, objective, n_iterations=20)\n",
    "print(\"Best Minsearch boost params:\", best_boost)\n",
    "print(\"Best validation MRR:\", best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c703900e",
   "metadata": {},
   "source": [
    "### 12. RAG Pipeline Evaluation and LLM Answer Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa827e",
   "metadata": {},
   "source": [
    "#### 12.1. Fixed Function Versions (no approach argument as in rag-flow.ipynb)\n",
    "\n",
    "In the context of evaluation, these functions are used to automate the process of generating answers for each ground-truth question, so that the answers can be judged (e.g., by the LLM-as-judge section) for relevance and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9149730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_minsearch(question):\n",
    "    search_results = ms_cover_then_hybrid_search(question, index, num_results=5)\n",
    "    prompt = build_prompt(question, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer\n",
    "\n",
    "def rag_elasticsearch(question):\n",
    "    search_results = es_cover_then_hybrid_search(question, num_results=5)\n",
    "    prompt = build_prompt(question, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada427e",
   "metadata": {},
   "source": [
    "#### 12.2. LLM-as-Judge Evaluation (RAG answer quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ae942",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "sample = df_gt.sample(n=50, random_state=1).to_dict(orient='records')\n",
    "evaluations_minsearch = []\n",
    "evaluations_es = []\n",
    "\n",
    "print(\"Evaluating RAG (Minsearch) with LLM-as-judge...\")\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag_minsearch(question)\n",
    "    prompt = prompt2_template.format(question=question, answer_llm=answer_llm)\n",
    "    evaluation = llm(prompt)\n",
    "    try:\n",
    "        evaluation = json.loads(evaluation)\n",
    "    except Exception:\n",
    "        evaluation = {\"Relevance\": \"ERROR\", \"Explanation\": evaluation}\n",
    "    evaluations_minsearch.append({\n",
    "        \"id\": record['id'],\n",
    "        \"question\": question,\n",
    "        \"answer\": answer_llm,\n",
    "        \"relevance\": evaluation.get(\"Relevance\"),\n",
    "        \"explanation\": evaluation.get(\"Explanation\")\n",
    "    })\n",
    "\n",
    "print(\"Evaluating RAG (Elasticsearch) with LLM-as-judge...\")\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag_elasticsearch(question)\n",
    "    prompt = prompt2_template.format(question=question, answer_llm=answer_llm)\n",
    "    evaluation = llm(prompt)\n",
    "    try:\n",
    "        evaluation = json.loads(evaluation)\n",
    "    except Exception:\n",
    "        evaluation = {\"Relevance\": \"ERROR\", \"Explanation\": evaluation}\n",
    "    evaluations_es.append({\n",
    "        \"id\": record['id'],\n",
    "        \"question\": question,\n",
    "        \"answer\": answer_llm,\n",
    "        \"relevance\": evaluation.get(\"Relevance\"),\n",
    "        \"explanation\": evaluation.get(\"Explanation\")\n",
    "    })\n",
    "\n",
    "df_eval_minsearch = pd.DataFrame(evaluations_minsearch)\n",
    "df_eval_es = pd.DataFrame(evaluations_es)\n",
    "df_eval_minsearch.to_csv('../data/rag-eval-minsearch.csv', index=False)\n",
    "df_eval_es.to_csv('../data/rag-eval-elasticsearch.csv', index=False)\n",
    "\n",
    "print(\"Minsearch RAG relevance proportions:\")\n",
    "print(df_eval_minsearch['relevance'].value_counts(normalize=True))\n",
    "print(\"Elasticsearch RAG relevance proportions:\")\n",
    "print(df_eval_es['relevance'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8657bd4",
   "metadata": {},
   "source": [
    "### 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40397505",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== RETRIEVAL METRICS ===\")\n",
    "print(\"Minsearch:\", metrics_minsearch)\n",
    "print(\"Elasticsearch:\", metrics_es)\n",
    "print(\"\\n=== RAG LLM-as-Judge (proportion RELEVANT) ===\")\n",
    "print(\"Minsearch:\", (df_eval_minsearch['relevance'] == 'RELEVANT').mean())\n",
    "print(\"Elasticsearch:\", (df_eval_es['relevance'] == 'RELEVANT').mean())\n",
    "print(\"\\nAll evaluation results saved to CSV in ../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e5c11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**How to interpret the LLM-as-Judge RAG evaluation output:**\n",
    "\n",
    "The table shows the proportions of answers classified as \"RELEVANT\" or \"PARTLY_RELEVANT\" by the LLM-as-judge for both Minsearch and Elasticsearch RAG pipelines.  \n",
    "- **RELEVANT:** Generated answers judged fully relevant to the user's question.\n",
    "- **PARTLY_RELEVANT:** Answers judged partially relevant.\n",
    "\n",
    "This relevance-based evaluation provides a more realistic measure of user experience than strict retrieval metrics\n",
    "\n",
    "---\n",
    "\n",
    "- **Retrieval metrics (Hit Rate, MRR)** and **LLM-as-judge** show the combined cover+hybrid+rerank approach is best for both Minsearch and Elasticsearch.\n",
    "- Minsearch is best for small datasets and ingredient coverage; Elasticsearch is best for scale.\n",
    "- All evaluation results are saved to CSV in `../data/`.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipe-assistant-P5TWTFE5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
