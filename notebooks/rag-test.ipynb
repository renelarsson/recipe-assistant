{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd519ef",
   "metadata": {},
   "source": [
    "### RAG Implementation and Search Setup\n",
    "\n",
    "This section sets up the core retrieval-augmented generation (RAG) workflow for the recipe assistant.  \n",
    "- **Download minsearch and Implement Basic Search:** We use the `minsearch` library to create a simple, in-memory search index over the recipe dataset.  \n",
    "- **Test minsearch retrieval:** We verify that the search index can retrieve relevant recipes based on a sample query.  \n",
    "- **Implement RAG flow:** We will combine retrieval (minsearch) with a large language model (LLM) to generate recipe recommendations based on user queries and the retrieved context.  \n",
    "- **Evaluate retrieval:** Later, we will add evaluation logic (e.g., hit rate, MRR) to measure how well the retrieval step surfaces relevant recipes.  \n",
    "- **Find best boost parameters:** We will experiment with boosting certain fields (like main ingredients) to improve retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d9d1595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘minsearch.py’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Download minsearch.py if not present\n",
    "!wget -nc https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37acd00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rll1405/recipe-assistant/notebooks/minsearch.py:10: UserWarning: Now minsearch is installable via pip: 'pip install minsearch'. Remove the downloaded file and re-install it with pip.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import minsearch\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/recipes_clean.csv')\n",
    "\n",
    "# Create documents for indexing\n",
    "documents = df.to_dict(orient='records')\n",
    "\n",
    "# Setup minsearch index\n",
    "index = minsearch.Index(\n",
    "    text_fields=['recipe_name', 'main_ingredients', 'all_ingredients', 'instructions', 'cuisine_type', 'dietary_restrictions'],\n",
    "    keyword_fields=['meal_type', 'difficulty_level']\n",
    ")\n",
    "\n",
    "# Fit the index\n",
    "index.fit(documents)\n",
    "\n",
    "# Test search\n",
    "query = \"chicken pasta italian\"\n",
    "results = index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2a2ba",
   "metadata": {},
   "source": [
    "### Implement OpenAI Integration and RAG Flow\n",
    "\n",
    "In this section, we integrate the OpenAI API to complete the RAG pipeline:\n",
    "- **Prompt Construction:** We define a function to build a prompt for the LLM using the context of the top retrieved recipes.\n",
    "- **LLM Call:** We define a function to call the OpenAI chat model with the constructed prompt.\n",
    "- **RAG Flow:** The `rag_flow` function ties everything together: it retrieves relevant recipes, builds the prompt, and gets a generated answer from the LLM.  \n",
    "This enables the assistant to provide recipe recommendations that are grounded in the actual recipe database, rather than relying solely on the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d293c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    " # Get access to OPENAI_API_KEY\n",
    "dotenv.load_dotenv(dotenv_path=\"../.env\") \n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    entry_template = \"\"\"\n",
    "Recipe: {recipe_name}\n",
    "Cuisine: {cuisine_type}\n",
    "Meal Type: {meal_type}\n",
    "Difficulty: {difficulty_level}\n",
    "Prep Time: {prep_time_minutes} minutes\n",
    "Cook Time: {cook_time_minutes} minutes\n",
    "Main Ingredients: {main_ingredients}\n",
    "Instructions: {instructions}\n",
    "Dietary Info: {dietary_restrictions}\n",
    "\"\"\".strip()\n",
    "    context = \"\\n\\n\".join([entry_template.format(**doc) for doc in search_results])\n",
    "    prompt_template = \"\"\"\n",
    "You are an expert chef and culinary assistant. Answer the question based on the content from our recipe database.\n",
    "Use only the facts from the context when answering the question.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide recipe recommendations with brief explanations of why they match the requested ingredients.\n",
    "If exact ingredients aren't available, suggest the closest matches and mention any substitutions needed.\n",
    "\"\"\".strip()\n",
    "    return prompt_template.format(context=context, question=query)\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag_flow(query):\n",
    "    search_results = index.search(query, num_results=5)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipe-assistant-P5TWTFE5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
