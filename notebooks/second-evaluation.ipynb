{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1919fb56",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) Evaluation for Recipe Assistant\n",
    "\n",
    "This notebook provides a comprehensive evaluation framework for the RAG system described in `rag-flow.ipynb`.  \n",
    "It covers ground-truth generation, retrieval metrics (Hit Rate, MRR), parameter optimization, and LLM-as-judge answer quality for both Minsearch and Elasticsearch using the best-performing combined retrieval and reranking approaches.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d6aa1",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports\n",
    "Import dependencies, load OpenAI API key and connect to OpenAI API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91136bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import dotenv\n",
    "import minsearch\n",
    "from elasticsearch import Elasticsearch\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a93e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(\"../.env\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d5858",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Preprocessing\n",
    "Load the recipe dataset from CSV file and prepare it for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad929e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/recipes_clean.csv')\n",
    "\n",
    "# Add an ID column if it doesn't exist\n",
    "if 'id' not in df.columns:\n",
    "    df['id'] = range(len(df))\n",
    "    \n",
    "# Create documents for indexing\n",
    "documents = df.to_dict(orient='records')\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f061a",
   "metadata": {},
   "source": [
    "### 3. Ground Truth Generation\n",
    "We generate ground-truth user questions for each recipe using the LLM.  \n",
    "This is used to evaluate retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26917b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You emulate a user of our recipe assistant application.\n",
    "Formulate 5 questions this user might ask based on a provided recipe.\n",
    "Make the questions specific to ingredients, cooking methods, \n",
    "cooking duration (prep/cook time), or dietary information in this recipe.\n",
    "Do NOT mention the recipe name in the question.\n",
    "The record should contain the answer to the questions, \n",
    "and the questions should be complete and not too short.\n",
    "Use as few words as possible from the record.\n",
    "\n",
    "The record:\n",
    "\n",
    "Recipe: {recipe_name}\n",
    "Cuisine: {cuisine_type}\n",
    "Main Ingredients: {main_ingredients}\n",
    "Instructions: {instructions}\n",
    "Dietary Info: {dietary_restrictions}\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks:\n",
    "\n",
    "{{\"questions\": [\"question1\", \"question2\", ..., \"question5\"]}}\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_questions(doc):\n",
    "    prompt = prompt_template.format(**doc)\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "results = {}\n",
    "for i, doc in enumerate(tqdm(documents[:100])):\n",
    "    doc_id = doc.get('id', i)\n",
    "    if doc_id in results:\n",
    "        continue\n",
    "    try:\n",
    "        questions_raw = generate_questions(doc)\n",
    "        questions = json.loads(questions_raw)\n",
    "        results[doc_id] = questions['questions']\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        continue\n",
    "\n",
    "final_results = []\n",
    "for doc_id, questions in results.items():\n",
    "    for q in questions:\n",
    "        final_results.append((doc_id, q))\n",
    "\n",
    "df_results = pd.DataFrame(final_results, columns=['id', 'question'])\n",
    "df_results.to_csv('../data/ground-truth-retrieval.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e791b5a6",
   "metadata": {},
   "source": [
    "### 4. Load Ground Truth Questions\n",
    "\n",
    "Load the generated ground-truth questions for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8eb9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt = pd.read_csv('../data/ground-truth-retrieval.csv')\n",
    "ground_truth = df_gt.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a7a09",
   "metadata": {},
   "source": [
    "### 5. Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28fff33",
   "metadata": {},
   "source": [
    "#### 5.1 Create Time Filter\n",
    "Users will be queried for available time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44187463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_max_time(results, max_time=None):\n",
    "    if max_time is None:\n",
    "        return results\n",
    "    filtered = []\n",
    "    for doc in results:\n",
    "        try:\n",
    "            total_time = int(doc.get('prep_time_minutes', 0)) + int(doc.get('cook_time_minutes', 0))\n",
    "        except Exception:\n",
    "            total_time = 99999\n",
    "        if total_time <= max_time:\n",
    "            filtered.append(doc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bfc45",
   "metadata": {},
   "source": [
    "#### 5.2 Print Unused Ingredients\n",
    "\n",
    "Utility function that, given the query and the results, prints which query ingredients were not covered by the selected recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0264404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize_ingredients is used in below Cover Ingredients Searches\n",
    "def tokenize_ingredients(ingredient_str):\n",
    "    # Split only on commas, strip whitespace, and lowercase\n",
    "    return set([ing.strip().lower() for ing in ingredient_str.split(',') if ing.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa77299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unused_ingredients(ingredients, results):\n",
    "    query_ings = tokenize_ingredients(ingredients)\n",
    "    used = set()\n",
    "    for doc in results:\n",
    "        recipe_ings = tokenize_ingredients(doc.get('all_ingredients', ''))\n",
    "        used |= recipe_ings\n",
    "    unused = query_ings - used\n",
    "    print(\"Unused ingredients:\", \", \".join(unused) if unused else \"All ingredients used!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33481ff7",
   "metadata": {},
   "source": [
    "#### 5.3. Deduplicate Search results\n",
    "Elasticsearch can sometimes return duplicate or near-duplicate documents in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_results(results):\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for doc in results:\n",
    "        # Use a unique field, e.g. 'id' or a tuple of fields\n",
    "        key = doc.get('id') or (doc.get('recipe_name'), doc.get('prep_time_minutes'), doc.get('cook_time_minutes'))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(doc)\n",
    "    return deduped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24269e97",
   "metadata": {},
   "source": [
    "### 5. Minsearch Setup\n",
    "Set up Minsearch retrieval backend as in the main RAG notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448c394",
   "metadata": {},
   "source": [
    "#### 5.1. Minsearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup minsearch index for recipes\n",
    "index = minsearch.Index(\n",
    "    text_fields=['recipe_name', 'main_ingredients', 'all_ingredients', \\\n",
    "                 'instructions', 'cuisine_type', 'dietary_restrictions'],\n",
    "    keyword_fields=['meal_type', 'difficulty_level']\n",
    ")\n",
    "\n",
    "# Fit/train the index on the recipe documents\n",
    "index.fit(documents)\n",
    "index.documents = index.docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05928346",
   "metadata": {},
   "source": [
    "#### 5.2. Elasticsearch Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6befe638",
   "metadata": {},
   "source": [
    "Run docker in terminal:\n",
    "\n",
    "`docker run -d --name elasticsearch -p 9200:9200 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:8.13.4`\n",
    "\n",
    "Or:\n",
    "\n",
    "`docker run -d --name elasticsearch \\\n",
    "  -p 9200:9200 \\\n",
    "  -e \"discovery.type=single-node\" \\\n",
    "  -e \"xpack.security.enabled=false\" \\\n",
    "  -e \"ES_JAVA_OPTS=-Xms512m -Xmx1g\" \\\n",
    "  docker.elastic.co/elasticsearch/elasticsearch:8.13.4`\n",
    "\n",
    "And check if Elasticsearch is up:\n",
    "\n",
    "`curl http://localhost:9200`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a22bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create Elasticsearch Client (make sure Elasticsearch is running locally)\n",
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b513183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Elasticsearch index settings and mappings for recipes\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1, # A unit of storage and search. More shards can improve parallelism for large datasets\n",
    "        \"number_of_replicas\": 0 # A shard copy for fault tolerance and increased search throughput (not recommended for production)\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"recipe_name\": {\"type\": \"text\"},\n",
    "            \"main_ingredients\": {\"type\": \"text\"},\n",
    "            \"all_ingredients\": {\"type\": \"text\"},\n",
    "            \"instructions\": {\"type\": \"text\"},\n",
    "            \"cuisine_type\": {\"type\": \"text\"},\n",
    "            \"dietary_restrictions\": {\"type\": \"text\"},\n",
    "            \"meal_type\": {\"type\": \"keyword\"},\n",
    "            \"difficulty_level\": {\"type\": \"keyword\"},\n",
    "            \"prep_time_minutes\": {\"type\": \"integer\"},\n",
    "            \"cook_time_minutes\": {\"type\": \"integer\"},\n",
    "            \"all_ingredients_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 1536  # text-embedding-3-small returns 1536-dimensional vectors\n",
    "}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"recipes\"\n",
    "\n",
    "# Create the index (ignore error if it already exists)\n",
    "try:\n",
    "    es_client.indices.create(index=index_name, body=index_settings)\n",
    "except Exception as e:\n",
    "    print(\"Index may already exist:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536,)\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=[text]\n",
    "    )\n",
    "    return np.array(response.data[0].embedding)\n",
    "\n",
    "emb = get_embedding(\"test\")\n",
    "print(emb.shape)  # Should print (1536,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b7bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cadc8b5ce0442eae7ff5e97801a4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/477 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Index documents into Elasticsearch, including the embedding vector\n",
    "for doc in tqdm(documents):\n",
    "    doc['all_ingredients_vector'] = get_embedding(doc['all_ingredients']).tolist()\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2e15f",
   "metadata": {},
   "source": [
    "### 7. Best Retrieval Strategies\n",
    "\n",
    "#### 7.1. Best Minsearch Retrieval and Rerank Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d5fe8",
   "metadata": {},
   "source": [
    "#### 7.2. Best Elasticsearch Retrieval and Rerank Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca3585",
   "metadata": {},
   "source": [
    "### 8. LLM-based Enhancements\n",
    "\n",
    "#### 8.1. Rerank with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3162012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_llm(query, candidates):\n",
    "    context = \"\\n\\n\".join([f\"Recipe: {doc['recipe_name']}\\nIngredients: {doc['main_ingredients']}\" for doc in candidates])\n",
    "    prompt = f\"\"\"\n",
    "Given the following user query and candidate recipes, rank the recipes from most to least relevant.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Candidates:\n",
    "{context}\n",
    "\n",
    "Return a JSON list of recipe names in ranked order.\n",
    "\"\"\".strip()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    import re\n",
    "    content = response.choices[0].message.content\n",
    "    json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
    "    if json_match:\n",
    "        ranked_names = json.loads(json_match.group())\n",
    "    else:\n",
    "        return candidates\n",
    "    ranked_docs = [doc for name in ranked_names for doc in candidates if doc['recipe_name'] == name]\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093d92f",
   "metadata": {},
   "source": [
    "#### 8.2. Query Rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2420ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(query):\n",
    "    prompt = f\"Rewrite this user query for a recipe search system to be more specific and clear: '{query}'\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e3752",
   "metadata": {},
   "source": [
    "### 9. Prompt Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_strategy_1():\n",
    "    return \"\"\"\n",
    "You are a chef assistant. Based on the available recipes, recommend dishes that use the requested ingredients.\n",
    "Provide the recipe name, brief description, and cooking instructions and time.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "def get_prompt_strategy_2():\n",
    "    return \"\"\"\n",
    "You are an expert chef specializing in ingredient substitutions. When users provide ingredients,\n",
    "recommend recipes and suggest alternatives for missing ingredients. Always explain possible substitutions\n",
    "and how they might affect the dish.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide recommendations with substitution suggestions:\n",
    "\"\"\".strip()\n",
    "\n",
    "def get_prompt_strategy_3():\n",
    "    return \"\"\"\n",
    "You are a nutritionist and chef. Recommend recipes based on ingredients provided, considering\n",
    "nutritional value and dietary restrictions. Highlight health benefits and suggest modifications\n",
    "for different dietary needs (vegetarian, gluten-free, etc.).\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide nutritionally-aware recommendations:\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b948d",
   "metadata": {},
   "source": [
    "### 10. Retrieval Evaluation: Hit Rate and MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6719d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt += 1\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank]:\n",
    "                total_score += 1 / (rank + 1)\n",
    "                break\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate_retrieval(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = str(q['id'])\n",
    "        results = search_function(q)\n",
    "        relevance = [str(d.get('id')) == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }\n",
    "\n",
    "print(\"Evaluating Minsearch retrieval...\")\n",
    "metrics_minsearch = evaluate_retrieval(ground_truth, lambda q: minsearch_search(q['question'], num_results=10))\n",
    "print(\"Minsearch:\", metrics_minsearch)\n",
    "\n",
    "print(\"Evaluating Elasticsearch retrieval...\")\n",
    "metrics_es = evaluate_retrieval(ground_truth, lambda q: elasticsearch_search(q['question'], num_results=10))\n",
    "print(\"Elasticsearch:\", metrics_es)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a028c95",
   "metadata": {},
   "source": [
    "### 11. Parameter Optimization for Minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f65448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_ranges = {\n",
    "    'recipe_name': (0.0, 1.0),\n",
    "    'main_ingredients': (0.0, 4.0),\n",
    "    'all_ingredients': (0.0, 5.0),\n",
    "    'instructions': (0.0, 3.0),\n",
    "    'cuisine_type': (0.0, 1.0),\n",
    "    'dietary_restrictions': (0.0, 2.0)\n",
    "}\n",
    "\n",
    "def simple_optimize(param_ranges, objective_function, n_iterations=10):\n",
    "    best_params = None\n",
    "    best_score = float('-inf')\n",
    "    for _ in range(n_iterations):\n",
    "        current_params = {}\n",
    "        for param, (min_val, max_val) in param_ranges.items():\n",
    "            current_params[param] = random.uniform(min_val, max_val)\n",
    "        current_score = objective_function(current_params)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_params = current_params\n",
    "    return best_params, best_score\n",
    "\n",
    "gt_val = df_gt.sample(n=50, random_state=42).to_dict(orient='records')\n",
    "\n",
    "def objective(boost_params):\n",
    "    def search_function(q):\n",
    "        return minsearch_search(q['question'], boost=boost_params, num_results=10)\n",
    "    results = evaluate_retrieval(gt_val, search_function)\n",
    "    return results['mrr']\n",
    "\n",
    "best_boost, best_score = simple_optimize(param_ranges, objective, n_iterations=20)\n",
    "print(\"Best Minsearch boost params:\", best_boost)\n",
    "print(\"Best validation MRR:\", best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c703900e",
   "metadata": {},
   "source": [
    "### 12. RAG Pipeline and LLM Answer Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9149730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    entry_template = \"\"\"\n",
    "Recipe: {recipe_name}\n",
    "Cuisine: {cuisine_type}\n",
    "Meal Type: {meal_type}\n",
    "Difficulty: {difficulty_level}\n",
    "Prep Time: {prep_time_minutes} minutes\n",
    "Cook Time: {cook_time_minutes} minutes\n",
    "Main Ingredients: {main_ingredients}\n",
    "Instructions: {instructions}\n",
    "Dietary Info: {dietary_restrictions}\n",
    "\"\"\".strip()\n",
    "    context = \"\\n\\n\".join([entry_template.format(**doc) for doc in search_results])\n",
    "    prompt_template = \"\"\"\n",
    "You are an expert chef and culinary assistant. Answer the question based on the content from \n",
    "our recipe database. Use only the facts from the context when answering the question.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide recipe recommendations with brief explanations of why they match the requested ingredients.\n",
    "If exact ingredients aren't available, suggest the closest matches and mention any substitutions needed.\n",
    "\"\"\".strip()\n",
    "    return prompt_template.format(context=context, question=query)\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag_minsearch(question):\n",
    "    search_results = minsearch_search(question, boost=best_boost, num_results=5)\n",
    "    prompt = build_prompt(question, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer\n",
    "\n",
    "def rag_elasticsearch(question):\n",
    "    search_results = elasticsearch_search(question, num_results=5)\n",
    "    prompt = build_prompt(question, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada427e",
   "metadata": {},
   "source": [
    "### 13. LLM-as-Judge Evaluation (RAG answer quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ae942",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "sample = df_gt.sample(n=50, random_state=1).to_dict(orient='records')\n",
    "evaluations_minsearch = []\n",
    "evaluations_es = []\n",
    "\n",
    "print(\"Evaluating RAG (Minsearch) with LLM-as-judge...\")\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag_minsearch(question)\n",
    "    prompt = prompt2_template.format(question=question, answer_llm=answer_llm)\n",
    "    evaluation = llm(prompt)\n",
    "    try:\n",
    "        evaluation = json.loads(evaluation)\n",
    "    except Exception:\n",
    "        evaluation = {\"Relevance\": \"ERROR\", \"Explanation\": evaluation}\n",
    "    evaluations_minsearch.append({\n",
    "        \"id\": record['id'],\n",
    "        \"question\": question,\n",
    "        \"answer\": answer_llm,\n",
    "        \"relevance\": evaluation.get(\"Relevance\"),\n",
    "        \"explanation\": evaluation.get(\"Explanation\")\n",
    "    })\n",
    "\n",
    "print(\"Evaluating RAG (Elasticsearch) with LLM-as-judge...\")\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag_elasticsearch(question)\n",
    "    prompt = prompt2_template.format(question=question, answer_llm=answer_llm)\n",
    "    evaluation = llm(prompt)\n",
    "    try:\n",
    "        evaluation = json.loads(evaluation)\n",
    "    except Exception:\n",
    "        evaluation = {\"Relevance\": \"ERROR\", \"Explanation\": evaluation}\n",
    "    evaluations_es.append({\n",
    "        \"id\": record['id'],\n",
    "        \"question\": question,\n",
    "        \"answer\": answer_llm,\n",
    "        \"relevance\": evaluation.get(\"Relevance\"),\n",
    "        \"explanation\": evaluation.get(\"Explanation\")\n",
    "    })\n",
    "\n",
    "df_eval_minsearch = pd.DataFrame(evaluations_minsearch)\n",
    "df_eval_es = pd.DataFrame(evaluations_es)\n",
    "df_eval_minsearch.to_csv('../data/rag-eval-minsearch.csv', index=False)\n",
    "df_eval_es.to_csv('../data/rag-eval-elasticsearch.csv', index=False)\n",
    "\n",
    "print(\"Minsearch RAG relevance proportions:\")\n",
    "print(df_eval_minsearch['relevance'].value_counts(normalize=True))\n",
    "print(\"Elasticsearch RAG relevance proportions:\")\n",
    "print(df_eval_es['relevance'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8657bd4",
   "metadata": {},
   "source": [
    "### 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40397505",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== RETRIEVAL METRICS ===\")\n",
    "print(\"Minsearch:\", metrics_minsearch)\n",
    "print(\"Elasticsearch:\", metrics_es)\n",
    "print(\"\\n=== RAG LLM-as-Judge (proportion RELEVANT) ===\")\n",
    "print(\"Minsearch:\", (df_eval_minsearch['relevance'] == 'RELEVANT').mean())\n",
    "print(\"Elasticsearch:\", (df_eval_es['relevance'] == 'RELEVANT').mean())\n",
    "print(\"\\nAll evaluation results saved to CSV in ../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e5c11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**How to interpret the LLM-as-Judge RAG evaluation output:**\n",
    "\n",
    "The table shows the proportions of answers classified as \"RELEVANT\" or \"PARTLY_RELEVANT\" by the LLM-as-judge for both Minsearch and Elasticsearch RAG pipelines.  \n",
    "- **RELEVANT:** Generated answers judged fully relevant to the user's question.\n",
    "- **PARTLY_RELEVANT:** Answers judged partially relevant.\n",
    "\n",
    "This relevance-based evaluation provides a more realistic measure of user experience than strict retrieval metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipe-assistant-P5TWTFE5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
