{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40928917",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Evaluation for Recipe Assistant\n",
    "\n",
    "This notebook provides a comprehensive evaluation framework for the RAG system described in `rag-flow.ipynb`.  \n",
    "It covers ground-truth generation, retrieval metrics (Hit Rate, MRR), parameter optimization, prompt strategies, and LLM-as-judge answer quality for both Minsearch and Elasticsearch using the best-performing combined retrieval and reranking approaches.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6565e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import dotenv\n",
    "import minsearch\n",
    "from elasticsearch import Elasticsearch\n",
    "from openai import OpenAI\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896936fe",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Load the recipe dataset from CSV file and prepare it for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/recipes_clean.csv')\n",
    "if 'id' not in df.columns:\n",
    "    df['id'] = range(len(df))\n",
    "documents = df.to_dict(orient='records')\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d6308",
   "metadata": {},
   "source": [
    "## 3. Ground Truth Generation\n",
    "\n",
    "We generate ground-truth user questions for each recipe using the LLM.  \n",
    "This is used to evaluate retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> The fields are the most relevant for generating realistic user questions, given the prompt instructions.\n",
    "prompt_template = \"\"\"\n",
    "You emulate a user of our recipe assistant application.\n",
    "Formulate 5 questions this user might ask based on a provided recipe.\n",
    "Make the questions specific to ingredients, cooking methods, \n",
    "cooking duration (total time), or dietary information in this recipe.\n",
    "Do NOT mention the recipe name in the question.\n",
    "The record should contain the answer to the questions, \n",
    "and the questions should be complete and not too short.\n",
    "Use as few words as possible from the record.\n",
    "\n",
    "The record:\n",
    "\n",
    "Recipe: {recipe_name}\n",
    "Cuisine: {cuisine_type}\n",
    "Main Ingredients: {main_ingredients}\n",
    "Instructions: {instructions}\n",
    "Dietary Info: {dietary_restrictions}\n",
    "Total Time: {total_time_minutes}\n",
    "Servings: {servings}\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks:\n",
    "\n",
    "{{\"questions\": [\"question1\", \"question2\", ..., \"question5\"]}}\n",
    "\"\"\".strip()\n",
    "\n",
    "for doc in documents:\n",
    "    try:\n",
    "        doc['total_time_minutes'] = int(doc.get('prep_time_minutes', 0)) + int(doc.get('cook_time_minutes', 0))\n",
    "    except Exception:\n",
    "        doc['total_time_minutes'] = \"\"\n",
    "\n",
    "def generate_questions(doc):\n",
    "    prompt = prompt_template.format(**doc)\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "results = {}\n",
    "for i, doc in enumerate(tqdm(documents[:100])):\n",
    "    doc_id = doc.get('id', i)\n",
    "    if doc_id in results:\n",
    "        continue\n",
    "    try:\n",
    "        questions_raw = generate_questions(doc)\n",
    "        questions = json.loads(questions_raw)\n",
    "        results[doc_id] = questions['questions']\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        continue\n",
    "\n",
    "final_results = []\n",
    "for doc_id, questions in results.items():\n",
    "    for q in questions:\n",
    "        final_results.append((doc_id, q))\n",
    "\n",
    "df_results = pd.DataFrame(final_results, columns=['id', 'question'])\n",
    "df_results.to_csv('../data/ground-truth-retrieval.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a0d15b",
   "metadata": {},
   "source": [
    "## 4. Load Ground Truth Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38292512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt = pd.read_csv('../data/ground-truth-retrieval.csv')\n",
    "ground_truth = df_gt.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eeea1b",
   "metadata": {},
   "source": [
    "## 5. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_max_time(results, max_time=None):\n",
    "    if max_time is None:\n",
    "        return results\n",
    "    filtered = []\n",
    "    for doc in results:\n",
    "        try:\n",
    "            total_time = int(doc.get('prep_time_minutes', 0)) + int(doc.get('cook_time_minutes', 0))\n",
    "        except Exception:\n",
    "            total_time = 99999\n",
    "        if total_time <= max_time:\n",
    "            filtered.append(doc)\n",
    "    return filtered\n",
    "\n",
    "def tokenize_ingredients(ingredient_str):\n",
    "    return set([ing.strip().lower() for ing in ingredient_str.split(',') if ing.strip()])\n",
    "\n",
    "def print_unused_ingredients(ingredients, results):\n",
    "    query_ings = tokenize_ingredients(ingredients)\n",
    "    used = set()\n",
    "    for doc in results:\n",
    "        recipe_ings = tokenize_ingredients(doc.get('all_ingredients', ''))\n",
    "        used |= recipe_ings\n",
    "    unused = query_ings - used\n",
    "    print(\"Unused ingredients:\", \", \".join(unused) if unused else \"All ingredients used!\")\n",
    "\n",
    "def deduplicate_results(results):\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for doc in results:\n",
    "        key = doc.get('id') or (doc.get('recipe_name'), doc.get('prep_time_minutes'), doc.get('cook_time_minutes'))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(doc)\n",
    "    return deduped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c45d1",
   "metadata": {},
   "source": [
    "## 6. Minsearch Setup and Combined RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=['recipe_name', 'main_ingredients', 'all_ingredients', 'instructions', 'cuisine_type', 'dietary_restrictions'],\n",
    "    keyword_fields=['meal_type', 'difficulty_level']\n",
    ")\n",
    "index.fit(documents)\n",
    "index.documents = index.docs\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=[text]\n",
    "    )\n",
    "    return np.array(response.data[0].embedding)\n",
    "\n",
    "if not hasattr(index, \"embeddings\"):\n",
    "    index.embeddings = [get_embedding(doc['all_ingredients']) for doc in index.documents]\n",
    "\n",
    "def ms_cover_ingredients_search(query, index, num_results=5, max_time=None):\n",
    "    query_ings = tokenize_ingredients(query)\n",
    "    uncovered = set(query_ings)\n",
    "    selected = []\n",
    "    docs = filter_by_max_time(index.documents.copy(), max_time)\n",
    "    while uncovered and len(selected) < num_results and docs:\n",
    "        best_doc = None\n",
    "        best_overlap = 0\n",
    "        for doc in docs:\n",
    "            recipe_ings = tokenize_ingredients(doc.get('all_ingredients', ''))\n",
    "            overlap = len(uncovered & recipe_ings)\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_doc = doc\n",
    "        if best_doc and best_overlap > 0:\n",
    "            selected.append(best_doc)\n",
    "            recipe_ings = tokenize_ingredients(best_doc.get('all_ingredients', ''))\n",
    "            uncovered -= recipe_ings\n",
    "            docs.remove(best_doc)\n",
    "        else:\n",
    "            break\n",
    "    deduped_results = deduplicate_results(selected)\n",
    "    return deduped_results\n",
    "\n",
    "def ms_cover_then_hybrid_search(query, index, num_results=5, max_time=None, hybrid_top_k=5):\n",
    "    cover_results = ms_cover_ingredients_search(query, index, num_results=num_results*4, max_time=max_time)\n",
    "    if not cover_results:\n",
    "        return []\n",
    "    query_emb = get_embedding(query)\n",
    "    cover_embeddings = [get_embedding(doc['all_ingredients']) for doc in cover_results]\n",
    "    similarities = [np.dot(query_emb, emb) for emb in cover_embeddings]\n",
    "    top_indices = np.argsort(similarities)[-hybrid_top_k:][::-1]\n",
    "    hybrid_results = [cover_results[i] for i in top_indices]\n",
    "    deduped_results = deduplicate_results(hybrid_results[:num_results])\n",
    "    return deduped_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7fb54",
   "metadata": {},
   "source": [
    "## 7. Elasticsearch Setup and Combined RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55511ae6",
   "metadata": {},
   "source": [
    "#### 7.1 (Re)starting Elasticsearch and Re-indexing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26308236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create Elasticsearch Client (make sure Elasticsearch is running locally)\n",
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e09e1",
   "metadata": {},
   "source": [
    "---\n",
    "If you have removed the Elasticsearch container (e.g., with `docker rm elasticsearch`), you must restart Elasticsearch and re-create the index and data.\n",
    "\n",
    "Run docker in terminal:\n",
    "\n",
    "`\n",
    "docker run -d --name elasticsearch -p 9200:9200 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:8.13.4\n",
    "`\n",
    "\n",
    "Or:\n",
    "\n",
    "`\n",
    "docker run -d --name elasticsearch \\\n",
    "  -p 9200:9200 \\\n",
    "  -e \"discovery.type=single-node\" \\\n",
    "  -e \"xpack.security.enabled=false\" \\\n",
    "  -e \"ES_JAVA_OPTS=-Xms512m -Xmx1g\" \\\n",
    "  docker.elastic.co/elasticsearch/elasticsearch:8.13.4\n",
    "`\n",
    "\n",
    "And check if Elasticsearch is up:\n",
    "\n",
    "`\n",
    "curl http://localhost:9200\n",
    "`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da3346",
   "metadata": {},
   "source": [
    "#### 7.2. Re-create the index with the correct settings and mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c97ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Elasticsearch index settings and mappings for recipes\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1, # A unit of storage and search. More shards can improve parallelism for large datasets\n",
    "        \"number_of_replicas\": 0 # A shard copy for fault tolerance and increased search throughput (not recommended for production)\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"recipe_name\": {\"type\": \"text\"},\n",
    "            \"main_ingredients\": {\"type\": \"text\"},\n",
    "            \"all_ingredients\": {\"type\": \"text\"},\n",
    "            \"instructions\": {\"type\": \"text\"},\n",
    "            \"cuisine_type\": {\"type\": \"text\"},\n",
    "            \"dietary_restrictions\": {\"type\": \"text\"},\n",
    "            \"meal_type\": {\"type\": \"keyword\"},\n",
    "            \"difficulty_level\": {\"type\": \"keyword\"},\n",
    "            \"prep_time_minutes\": {\"type\": \"integer\"},\n",
    "            \"cook_time_minutes\": {\"type\": \"integer\"},\n",
    "            \"all_ingredients_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 1536  # text-embedding-3-small returns 1536-dimensional vectors\n",
    "}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"recipes\"\n",
    "\n",
    "# Create the index (ignore error if it already exists)\n",
    "try:\n",
    "    es_client.indices.create(index=index_name, body=index_settings)\n",
    "except Exception as e:\n",
    "    print(\"Index may already exist:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Re-index all your documents (including the embedding vector)\n",
    "for doc in tqdm(documents):\n",
    "    doc['all_ingredients_vector'] = get_embedding(doc['all_ingredients']).tolist()\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_basic_search(query, num_results=5, max_time=None):\n",
    "    search_query = {\n",
    "        \"size\": num_results * 2,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\n",
    "                    \"recipe_name\",\n",
    "                    \"main_ingredients^2\",\n",
    "                    \"all_ingredients^3\",\n",
    "                    \"instructions^1.5\",\n",
    "                    \"cuisine_type\",\n",
    "                    \"dietary_restrictions^1.5\"\n",
    "                ],\n",
    "                \"type\": \"best_fields\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    result_docs = [hit['_source'] for hit in response['hits']['hits']]\n",
    "    if max_time is not None:\n",
    "        result_docs = filter_by_max_time(result_docs, max_time)\n",
    "    return result_docs[:num_results]\n",
    "\n",
    "def es_cover_ingredients_search(query, num_results=5, max_time=None, candidate_pool_size=200):\n",
    "    candidates = es_basic_search(query, num_results=candidate_pool_size)\n",
    "    if max_time is not None:\n",
    "        candidates = filter_by_max_time(candidates, max_time)\n",
    "    query_tokens = set(re.sub(r'[^\\w\\s]', '', query.lower()).replace(',', ' ').split())\n",
    "    uncovered = set(query_tokens)\n",
    "    selected = []\n",
    "    docs = candidates.copy()\n",
    "    while uncovered and len(selected) < num_results and docs:\n",
    "        best_doc = None\n",
    "        best_overlap = 0\n",
    "        for doc in docs:\n",
    "            ingredients = set(re.sub(r'[^\\w\\s]', '', str(doc.get('all_ingredients', '')).lower()).replace(',', ' ').split())\n",
    "            overlap = len(uncovered & ingredients)\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_doc = doc\n",
    "        if best_doc and best_overlap > 0:\n",
    "            selected.append(best_doc)\n",
    "            ingredients = set(re.sub(r'[^\\w\\s]', '', str(best_doc.get('all_ingredients', '')).lower()).replace(',', ' ').split())\n",
    "            uncovered -= ingredients\n",
    "            docs.remove(best_doc)\n",
    "        else:\n",
    "            break\n",
    "    deduped_results = deduplicate_results(selected)\n",
    "    return deduped_results\n",
    "\n",
    "def es_cover_then_hybrid_search(query, num_results=5, max_time=None, hybrid_top_k=5, candidate_pool_size=200):\n",
    "    cover_results = es_cover_ingredients_search(query, num_results=candidate_pool_size, max_time=max_time)\n",
    "    if not cover_results:\n",
    "        return []\n",
    "    query_emb = get_embedding(query)\n",
    "    cover_embeddings = [get_embedding(doc['all_ingredients']) for doc in cover_results]\n",
    "    similarities = [np.dot(query_emb, emb) for emb in cover_embeddings]\n",
    "    top_indices = np.argsort(similarities)[-hybrid_top_k:][::-1]\n",
    "    hybrid_results = [cover_results[i] for i in top_indices]\n",
    "    for doc in hybrid_results:\n",
    "        doc.pop('all_ingredients_vector', None)\n",
    "    deduped_results = deduplicate_results(hybrid_results[:num_results])\n",
    "    return deduped_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8cba6",
   "metadata": {},
   "source": [
    "## 8. Prompt Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_strategy_1():\n",
    "    return \"\"\"\n",
    "You are a chef assistant. Based on the available recipes, recommend dishes that use the requested ingredients.\n",
    "Provide the recipe name, brief description, and cooking instructions and time.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "def get_prompt_strategy_2():\n",
    "    return \"\"\"\n",
    "You are an expert chef specializing in ingredient substitutions. When users provide ingredients,\n",
    "recommend recipes and suggest alternatives for missing ingredients. Always explain possible substitutions\n",
    "and how they might affect the dish.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide recommendations with substitution suggestions:\n",
    "\"\"\".strip()\n",
    "\n",
    "def get_prompt_strategy_3():\n",
    "    return \"\"\"\n",
    "You are a nutritionist and chef. Recommend recipes based on ingredients provided, considering\n",
    "nutritional value and dietary restrictions. Highlight health benefits and suggest modifications\n",
    "for different dietary needs (vegetarian, gluten-free, etc.).\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide nutritionally-aware recommendations:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f944cda",
   "metadata": {},
   "source": [
    "## 9. Build Prompt and LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21943175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    entry_template = \"\"\"\n",
    "Recipe: {recipe_name}\n",
    "Cuisine: {cuisine_type}\n",
    "Meal Type: {meal_type}\n",
    "Difficulty: {difficulty_level}\n",
    "Prep Time: {prep_time_minutes} minutes\n",
    "Cook Time: {cook_time_minutes} minutes\n",
    "Main Ingredients: {main_ingredients}\n",
    "Instructions: {instructions}\n",
    "Dietary Info: {dietary_restrictions}\n",
    "\"\"\".strip()\n",
    "    context = \"\\n\\n\".join([entry_template.format(**doc) for doc in search_results])\n",
    "    prompt_template = \"\"\"\n",
    "You are an expert chef and culinary assistant. Answer the question based on the content from our recipe database.\n",
    "Use only the facts from the context when answering the question.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide recipe recommendations with brief explanations of why they match the requested ingredients.\n",
    "If exact ingredients aren't available, suggest the closest matches and mention any substitutions needed.\n",
    "\"\"\".strip()\n",
    "    return prompt_template.format(context=context, question=query)\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e2915",
   "metadata": {},
   "source": [
    "## 10. Reranking with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef91ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_llm(query, candidates, max_time=None):\n",
    "    candidates = deduplicate_results(candidates)\n",
    "    if max_time is not None:\n",
    "        candidates = filter_by_max_time(candidates, max_time)\n",
    "    context = \"\\n\\n\".join([f\"Recipe: {doc['recipe_name']}\\nIngredients: {doc['main_ingredients']}\" for doc in candidates])\n",
    "    prompt = f\"\"\"\n",
    "Given the following user query and candidate recipes, rank the recipes from most to least relevant.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Candidates:\n",
    "{context}\n",
    "\n",
    "Return a JSON list of recipe names in ranked order.\n",
    "\"\"\".strip()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
    "    if json_match:\n",
    "        ranked_names = json.loads(json_match.group())\n",
    "    else:\n",
    "        return candidates\n",
    "    ranked_docs = [doc for name in ranked_names for doc in candidates if doc['recipe_name'] == name]\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846e852",
   "metadata": {},
   "source": [
    "## 11. Retrieval Evaluation: Hit Rate and MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6fa2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt += 1\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank]:\n",
    "                total_score += 1 / (rank + 1)\n",
    "                break\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate_retrieval(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = str(q['id'])\n",
    "        results = search_function(q)\n",
    "        relevance = [str(d.get('id')) == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }\n",
    "\n",
    "print(\"Evaluating Minsearch retrieval...\")\n",
    "metrics_minsearch = evaluate_retrieval(ground_truth, lambda q: ms_cover_then_hybrid_search(q['question'], index, num_results=10))\n",
    "print(\"Minsearch:\", metrics_minsearch)\n",
    "\n",
    "print(\"Evaluating Elasticsearch retrieval...\")\n",
    "metrics_es = evaluate_retrieval(ground_truth, lambda q: es_cover_then_hybrid_search(q['question'], num_results=10))\n",
    "print(\"Elasticsearch:\", metrics_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e768449",
   "metadata": {},
   "source": [
    "## 12. Parameter Optimization for Minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d1a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_ranges = { # Used for optimizing text field boosts in Minsearch\n",
    "    'recipe_name': (0.0, 1.0),\n",
    "    'main_ingredients': (0.0, 4.0),\n",
    "    'all_ingredients': (0.0, 5.0),\n",
    "    'instructions': (0.0, 3.0),\n",
    "    'cuisine_type': (0.0, 1.0),\n",
    "    'dietary_restrictions': (0.0, 2.0)\n",
    "}\n",
    "\n",
    "def simple_optimize(param_ranges, objective_function, n_iterations=10):\n",
    "    best_params = None\n",
    "    best_score = float('-inf')\n",
    "    for _ in range(n_iterations):\n",
    "        current_params = {}\n",
    "        for param, (min_val, max_val) in param_ranges.items():\n",
    "            current_params[param] = random.uniform(min_val, max_val)\n",
    "        current_score = objective_function(current_params)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_params = current_params\n",
    "    return best_params, best_score\n",
    "\n",
    "gt_val = df_gt.sample(n=50, random_state=42).to_dict(orient='records')\n",
    "\n",
    "def objective(boost_params):\n",
    "    def search_function(q):\n",
    "        return index.search(q['question'], boost_dict=boost_params, num_results=10)\n",
    "    results = evaluate_retrieval(gt_val, search_function)\n",
    "    return results['mrr']\n",
    "\n",
    "best_boost, best_score = simple_optimize(param_ranges, objective, n_iterations=20)\n",
    "print(\"Best Minsearch boost params:\", best_boost)\n",
    "print(\"Best validation MRR:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e63b4",
   "metadata": {},
   "source": [
    "## 13. RAG Pipeline Evaluation and LLM Answer Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c03b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_minsearch(question):\n",
    "    search_results = ms_cover_then_hybrid_search(question, index, num_results=5)\n",
    "    prompt = build_prompt(question, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer\n",
    "\n",
    "def rag_elasticsearch(question):\n",
    "    search_results = es_cover_then_hybrid_search(question, num_results=5)\n",
    "    prompt = build_prompt(question, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956e8eb",
   "metadata": {},
   "source": [
    "## 14. LLM-as-Judge Evaluation (RAG answer quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "sample = df_gt.sample(n=50, random_state=1).to_dict(orient='records')\n",
    "evaluations_minsearch = []\n",
    "evaluations_es = []\n",
    "\n",
    "print(\"Evaluating RAG (Minsearch) with LLM-as-judge...\")\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag_minsearch(question)\n",
    "    prompt = prompt2_template.format(question=question, answer_llm=answer_llm)\n",
    "    evaluation = llm(prompt)\n",
    "    try:\n",
    "        evaluation = json.loads(evaluation)\n",
    "    except Exception:\n",
    "        evaluation = {\"Relevance\": \"ERROR\", \"Explanation\": evaluation}\n",
    "    evaluations_minsearch.append({\n",
    "        \"id\": record['id'],\n",
    "        \"question\": question,\n",
    "        \"answer\": answer_llm,\n",
    "        \"relevance\": evaluation.get(\"Relevance\"),\n",
    "        \"explanation\": evaluation.get(\"Explanation\")\n",
    "    })\n",
    "\n",
    "print(\"Evaluating RAG (Elasticsearch) with LLM-as-judge...\")\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag_elasticsearch(question)\n",
    "    prompt = prompt2_template.format(question=question, answer_llm=answer_llm)\n",
    "    evaluation = llm(prompt)\n",
    "    try:\n",
    "        evaluation = json.loads(evaluation)\n",
    "    except Exception:\n",
    "        evaluation = {\"Relevance\": \"ERROR\", \"Explanation\": evaluation}\n",
    "    evaluations_es.append({\n",
    "        \"id\": record['id'],\n",
    "        \"question\": question,\n",
    "        \"answer\": answer_llm,\n",
    "        \"relevance\": evaluation.get(\"Relevance\"),\n",
    "        \"explanation\": evaluation.get(\"Explanation\")\n",
    "    })\n",
    "\n",
    "df_eval_minsearch = pd.DataFrame(evaluations_minsearch)\n",
    "df_eval_es = pd.DataFrame(evaluations_es)\n",
    "df_eval_minsearch.to_csv('../data/rag-eval-minsearch.csv', index=False)\n",
    "df_eval_es.to_csv('../data/rag-eval-elasticsearch.csv', index=False)\n",
    "\n",
    "print(\"Minsearch RAG relevance proportions:\")\n",
    "print(df_eval_minsearch['relevance'].value_counts(normalize=True))\n",
    "print(\"Elasticsearch RAG relevance proportions:\")\n",
    "print(df_eval_es['relevance'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de680f47",
   "metadata": {},
   "source": [
    "## 15. Summary\n",
    "\n",
    "- **Retrieval metrics (Hit Rate, MRR)** and **LLM-as-judge** show the combined cover+hybrid+rerank approach is best for both Minsearch and Elasticsearch.\n",
    "- Minsearch is best for small datasets and ingredient coverage; Elasticsearch is best for scale.\n",
    "- All evaluation results are saved to CSV in `../data/`.\n",
    "\n",
    "---\n",
    "\n",
    "**How to interpret the LLM-as-Judge RAG evaluation output:**  \n",
    "The table shows the proportions of answers classified as \"RELEVANT\" or \"PARTLY_RELEVANT\" by the LLM-as-judge for both Minsearch and Elasticsearch RAG pipelines.  \n",
    "This relevance-based evaluation provides a more realistic measure of user experience than strict retrieval metrics alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipe-assistant-P5TWTFE5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
