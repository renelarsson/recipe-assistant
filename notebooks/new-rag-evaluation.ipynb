{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a8e2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup and Imports ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import dotenv\n",
    "import minsearch\n",
    "from elasticsearch import Elasticsearch\n",
    "from openai import OpenAI\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "887d61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Data Loading and Preprocessing ---\n",
    "\n",
    "df = pd.read_csv('../data/recipes_clean.csv')\n",
    "\n",
    "# Ensure that every recipe/document can be uniquely identified to track which recipe a question or answer belongs to\n",
    "if 'id' not in df.columns:\n",
    "    df['id'] = range(len(df))\n",
    "documents = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d36680ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee2aa6e3147414e8ff08ce3ca12c520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 3. Ground Truth Generation (if not already present) ---\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You emulate a user of our recipe assistant application.\n",
    "Formulate 5 questions this user might ask based on a provided recipe.\n",
    "Make the questions specific to ingredients, cooking methods, \n",
    "cooking duration (prep/cook time), or dietary information in this recipe.\n",
    "Do NOT mention the recipe name in the question.\n",
    "The record should contain the answer to the questions, \n",
    "and the questions should be complete and not too short.\n",
    "Use as few words as possible from the record.\n",
    "\n",
    "The record:\n",
    "\n",
    "Recipe: {recipe_name}\n",
    "Cuisine: {cuisine_type}\n",
    "Main Ingredients: {main_ingredients}\n",
    "Instructions: {instructions}\n",
    "Dietary Info: {dietary_restrictions}\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks:\n",
    "\n",
    "{{\"questions\": [\"question1\", \"question2\", ..., \"question5\"]}}\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_questions(doc):\n",
    "    prompt = prompt_template.format(**doc)\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Only run if you need to regenerate ground truth\n",
    "results = {}\n",
    "for i, doc in enumerate(tqdm(documents[:100])):\n",
    "    doc_id = doc.get('id', i)\n",
    "    if doc_id in results:\n",
    "        continue\n",
    "    try:\n",
    "        questions_raw = generate_questions(doc)\n",
    "        questions = json.loads(questions_raw)\n",
    "        results[doc_id] = questions['questions']\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        continue\n",
    "final_results = []\n",
    "for doc_id, questions in results.items():\n",
    "    for q in questions:\n",
    "        final_results.append((doc_id, q))\n",
    "df_results = pd.DataFrame(final_results, columns=['id', 'question'])\n",
    "df_results.to_csv('../data/ground-truth-retrieval.csv', index=False)\n",
    "\n",
    "df_gt = pd.read_csv('../data/ground-truth-retrieval.csv')\n",
    "ground_truth = df_gt.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2f974",
   "metadata": {},
   "source": [
    "Run terminal command if the container already exists:\n",
    "\n",
    "`docker start elasticsearch`\n",
    "\n",
    "Run terminal command:\n",
    "\n",
    "`docker run -d --name elasticsearch -p 9200:9200 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:8.13.4`\n",
    "\n",
    "Or:\n",
    "\n",
    "`docker run -d --name elasticsearch \\\n",
    "  -p 9200:9200 \\\n",
    "  -e \"discovery.type=single-node\" \\\n",
    "  -e \"xpack.security.enabled=false\" \\\n",
    "  -e \"ES_JAVA_OPTS=-Xms512m -Xmx1g\" \\\n",
    "  docker.elastic.co/elasticsearch/elasticsearch:8.13.4`\n",
    "\n",
    "And check if Elasticsearch is up:\n",
    "\n",
    "`curl http://localhost:9200`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65c1cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Minsearch and Elasticsearch Setup (from rag-flow.ipynb) ---\n",
    "\n",
    "# Minsearch\n",
    "index = minsearch.Index(\n",
    "    text_fields=['recipe_name', 'main_ingredients', 'all_ingredients', 'instructions', \n",
    "                 'cuisine_type', 'dietary_restrictions'],\n",
    "    keyword_fields=['meal_type', 'difficulty_level']\n",
    ")\n",
    "index.fit(documents)\n",
    "\n",
    "def minsearch_search(query, boost=None, num_results=10):\n",
    "    if boost is None:\n",
    "        boost = {'main_ingredients': 4.0, 'all_ingredients': 5.0, 'instructions': 3.0,\n",
    "                 'cuisine_type': 1.0, 'dietary_restrictions': 2.0}\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        boost_dict=boost,\n",
    "        num_results=num_results\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200')\n",
    "index_name = \"recipes\"\n",
    "# 10 results for more robust retrieval evaluation\n",
    "def elasticsearch_search(query, num_results=10):\n",
    "    search_query = {\n",
    "        \"size\": num_results,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\n",
    "                    \"recipe_name\",\n",
    "                    \"main_ingredients^4\",\n",
    "                    \"all_ingredients^5\",\n",
    "                    \"instructions^3\",\n",
    "                    \"cuisine_type\",\n",
    "                    \"dietary_restrictions^2\"\n",
    "                ],\n",
    "                \"type\": \"best_fields\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    result_docs = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbca4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Retrieval Evaluation: Hit Rate and MRR ---\n",
    "\n",
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt += 1\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank]:\n",
    "                total_score += 1 / (rank + 1)\n",
    "                break\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate_retrieval(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = str(q['id'])\n",
    "        results = search_function(q)  # Pass the whole dict\n",
    "        relevance = [str(d.get('id')) == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8f33e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Minsearch retrieval...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95da6852e96247848595606c9a77256e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minsearch: {'hit_rate': 0.206, 'mrr': 0.10825555555555563}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Minsearch retrieval...\")\n",
    "metrics_minsearch = evaluate_retrieval(ground_truth, lambda q: minsearch_search(q['question'], num_results=10))\n",
    "print(\"Minsearch:\", metrics_minsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7906e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recipe_name': 'Spaghetti Carbonara', 'cuisine_type': 'Italian', 'meal_type': 'Dinner', 'difficulty_level': 'Medium', 'prep_time_minutes': 15, 'cook_time_minutes': 20, 'servings': 4, 'main_ingredients': 'Spaghetti, Eggs, Pancetta, Parmesan', 'all_ingredients': 'Spaghetti, Eggs, Pancetta, Parmesan, Black Pepper, Olive Oil, Salt', 'dietary_restrictions': 'Contains gluten, dairy, pork', 'instructions': 'Boil spaghetti until al dente. Fry pancetta until crispy. Whisk eggs with grated Parmesan. Toss hot spaghetti with pancetta and egg mixture off heat. Serve immediately with black pepper.', 'nutritional_info': 'Calories: 520, Protein: 22g, Carbs: 60g, Fat: 22g', 'id': 0}\n"
     ]
    }
   ],
   "source": [
    "# Re-index: send each document with 'id' to Elasticsearch\n",
    "for doc in documents:\n",
    "    es_client.index(index=\"recipes\", document=doc)\n",
    "    \n",
    "# Look for 'id' field in the indexed documents\n",
    "print(es_client.search(index=\"recipes\", body={\"size\": 1, \"query\": {\"match_all\": {}}})['hits']['hits'][0]['_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2767f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Elasticsearch retrieval...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b993047667b942fca3515a3a7ceedd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearch: {'hit_rate': 0.232, 'mrr': 0.10885634920634935}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Elasticsearch retrieval...\")\n",
    "metrics_es = evaluate_retrieval(ground_truth, lambda q: elasticsearch_search(q['question'], num_results=10))\n",
    "print(\"Elasticsearch:\", metrics_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297297d",
   "metadata": {},
   "source": [
    "**Minsearch:**\n",
    "\n",
    "* hit_rate = 0.206: For 20.6% of the questions, the exact ground truth recipe was found in the top 10 results.\n",
    "* mrr = 0.108: On average, the ground truth recipe appeared lower in the ranking (higher is better; 1.0 means always ranked #1, 0.0 means never found).\n",
    "\n",
    "**Elasticsearch:**\n",
    "\n",
    "* hit_rate = 0.232: For 23.2% of the questions, the exact ground truth recipe was found in the top 10 results.\n",
    "* mrr = 0.109: On average, the ground truth recipe was ranked similarly to Minsearch.\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "These lower hit rate and MRR values are expected because the user queries focus on ingredients, cooking methods, and dietary information, not on recipe names. In this setting, many recipes may be plausible answers for a given query, and the \"ground truth\" recipe is just one of several valid possibilities. Therefore, these metrics are a strict measure and may underestimate the true usefulness of the retrieval system for users.\n",
    "\n",
    "For ingredient-based or attribute-based search, user satisfaction often depends on retrieving any relevant recipe, not necessarily the exact ground truth. To better assess real-world performance, we will consider supplementing these metrics with relevance-based evaluation (e.g., LLM-as-judge) to capture how well the retrieved recipes actually match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea67a631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7304e53bdbd5498baa26338b634bce92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e13c384ba264073bc65c8e34ab18eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3b148043d54f50a25c5be6aee701cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180e853da48c4477a3a032df7488e332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91be98aa2f9847f993edfe2b5ac66e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906c868c217443758e289ab2313d5094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8829c8e3458740eebd908926c76636f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9b190bb93e46949d318e9ce25db47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c461d2275eb0471dbe2d11856a702851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee26ecd5779409296a6c35adcbda973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9a3a883adf44abb11fc4620745fe4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2dd80ca6374e04bb0fb9d4d681a855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6595e830df2341afb3fff05177306924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13de7cc36a3425abffb867d673e73dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5db3fc0d8a74eeaaf0fa4481825bfe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0803533350b048eb991f263c867cbd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59454f450ca4977adb49659a556afc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84a7b55776d4cfdaadbe4a5fb618abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7374ac8df1b4626bbad657ea2a56bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8309f1a672ed4eb68ab58b113d771a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Minsearch boost params: {'recipe_name': 0.04466476234708505, 'main_ingredients': 1.8510064727585926, 'all_ingredients': 2.3127319210696817, 'instructions': 1.6169840103279864, 'cuisine_type': 0.9784550595609097, 'dietary_restrictions': 1.3399640700511697}\n",
      "Best validation MRR: 0.09688888888888889\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Parameter Optimization for Minsearch ---\n",
    "\n",
    "import random\n",
    "\n",
    "param_ranges = {\n",
    "    'recipe_name': (0.0, 1.0),\n",
    "    'main_ingredients': (0.0, 4.0),\n",
    "    'all_ingredients': (0.0, 5.0),\n",
    "    'instructions': (0.0, 3.0),\n",
    "    'cuisine_type': (0.0, 1.0),\n",
    "    'dietary_restrictions': (0.0, 2.0)\n",
    "}\n",
    "\n",
    "def simple_optimize(param_ranges, objective_function, n_iterations=10):\n",
    "    best_params = None\n",
    "    best_score = float('-inf')\n",
    "    for _ in range(n_iterations):\n",
    "        current_params = {}\n",
    "        for param, (min_val, max_val) in param_ranges.items():\n",
    "            current_params[param] = random.uniform(min_val, max_val)\n",
    "        current_score = objective_function(current_params)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_params = current_params\n",
    "    return best_params, best_score\n",
    "\n",
    "gt_val = df_gt.sample(n=50, random_state=42).to_dict(orient='records')\n",
    "\n",
    "def objective(boost_params):\n",
    "    def search_function(q):\n",
    "        # q is a dict with keys 'id' and 'question'\n",
    "        return minsearch_search(q['question'], boost=boost_params, num_results=10)\n",
    "    results = evaluate_retrieval(gt_val, search_function)\n",
    "    return results['mrr']\n",
    "\n",
    "best_boost, best_score = simple_optimize(param_ranges, objective, n_iterations=20)\n",
    "\n",
    "print(\"Best Minsearch boost params:\", best_boost)\n",
    "print(\"Best validation MRR:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730360c4",
   "metadata": {},
   "source": [
    "**How to interpret the parameter optimization output:**\n",
    "\n",
    "The \"Best Minsearch boost params\" are the weights for each field that gave the highest mean reciprocal rank (MRR) on the validation set. A higher weight means that field contributed more to matching queries to recipes. The \"Best validation MRR\" shows how well the retrieval ranked the ground truth recipe on average. \n",
    "\n",
    "A low MRR is expected for ingredient-based queries, since many recipes may be plausible answers and the metric only rewards finding the exact ground truth. The optimal parameters found may not always outperform your defaults due to randomness, small validation size, or the strictness of the metric. For a fuller picture, we will supplement with relevance-based evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a41e4a",
   "metadata": {},
   "source": [
    "This cell generates answers using the RAG pipeline from the rag-flow notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72018038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. RAG Pipeline Evaluation (LLM answer quality) ---\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    entry_template = \"\"\"\n",
    "Recipe: {recipe_name}\n",
    "Cuisine: {cuisine_type}\n",
    "Meal Type: {meal_type}\n",
    "Difficulty: {difficulty_level}\n",
    "Prep Time: {prep_time_minutes} minutes\n",
    "Cook Time: {cook_time_minutes} minutes\n",
    "Main Ingredients: {main_ingredients}\n",
    "Instructions: {instructions}\n",
    "Dietary Info: {dietary_restrictions}\n",
    "\"\"\".strip()\n",
    "    context = \"\\n\\n\".join([entry_template.format(**doc) for doc in search_results])\n",
    "    prompt_template = \"\"\"\n",
    "You are an expert chef and culinary assistant. Answer the question based on the content from \n",
    "our recipe database. Use only the facts from the context when answering the question.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Provide recipe recommendations with brief explanations of why they match the requested ingredients.\n",
    "If exact ingredients aren't available, suggest the closest matches and mention any substitutions needed.\n",
    "\"\"\".strip()\n",
    "    return prompt_template.format(context=context, question=query)\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag_minsearch(question):\n",
    "    search_results = minsearch_search(question, boost=best_boost, num_results=5)\n",
    "    prompt = build_prompt(question, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer\n",
    "\n",
    "def rag_elasticsearch(question):\n",
    "    search_results = elasticsearch_search(question, num_results=5)\n",
    "    prompt = build_prompt(question, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488e3c5",
   "metadata": {},
   "source": [
    "This cell evaluates the quality of the answers generated by the RAG pipeline. It samples questions, generates answers using the RAG functions, and then asks the LLM to judge the relevance of each answer.\n",
    "The LLM then classifys each answer as \"RELEVANT\", \"PARTLY_RELEVANT\", or \"NON_RELEVANT\" with explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3164d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RAG (Minsearch) with LLM-as-judge...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10340e68a6604064b41e0b0e2634e56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RAG (Elasticsearch) with LLM-as-judge...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab26a17b79c4200836fac96c2578349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minsearch RAG relevance proportions:\n",
      "relevance\n",
      "RELEVANT           0.78\n",
      "PARTLY_RELEVANT    0.22\n",
      "Name: proportion, dtype: float64\n",
      "Elasticsearch RAG relevance proportions:\n",
      "relevance\n",
      "RELEVANT           0.78\n",
      "PARTLY_RELEVANT    0.22\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- 8. LLM-as-Judge Evaluation (RAG answer quality) ---\n",
    "\n",
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "sample = df_gt.sample(n=50, random_state=1).to_dict(orient='records')\n",
    "evaluations_minsearch = []\n",
    "evaluations_es = []\n",
    "\n",
    "print(\"Evaluating RAG (Minsearch) with LLM-as-judge...\")\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag_minsearch(question)\n",
    "    prompt = prompt2_template.format(question=question, answer_llm=answer_llm)\n",
    "    evaluation = llm(prompt)\n",
    "    try:\n",
    "        evaluation = json.loads(evaluation)\n",
    "    except Exception:\n",
    "        evaluation = {\"Relevance\": \"ERROR\", \"Explanation\": evaluation}\n",
    "    evaluations_minsearch.append({\n",
    "        \"id\": record['id'],\n",
    "        \"question\": question,\n",
    "        \"answer\": answer_llm,\n",
    "        \"relevance\": evaluation.get(\"Relevance\"),\n",
    "        \"explanation\": evaluation.get(\"Explanation\")\n",
    "    })\n",
    "\n",
    "print(\"Evaluating RAG (Elasticsearch) with LLM-as-judge...\")\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag_elasticsearch(question)\n",
    "    prompt = prompt2_template.format(question=question, answer_llm=answer_llm)\n",
    "    evaluation = llm(prompt)\n",
    "    try:\n",
    "        evaluation = json.loads(evaluation)\n",
    "    except Exception:\n",
    "        evaluation = {\"Relevance\": \"ERROR\", \"Explanation\": evaluation}\n",
    "    evaluations_es.append({\n",
    "        \"id\": record['id'],\n",
    "        \"question\": question,\n",
    "        \"answer\": answer_llm,\n",
    "        \"relevance\": evaluation.get(\"Relevance\"),\n",
    "        \"explanation\": evaluation.get(\"Explanation\")\n",
    "    })\n",
    "\n",
    "df_eval_minsearch = pd.DataFrame(evaluations_minsearch)\n",
    "df_eval_es = pd.DataFrame(evaluations_es)\n",
    "df_eval_minsearch.to_csv('../data/rag-eval-minsearch.csv', index=False)\n",
    "df_eval_es.to_csv('../data/rag-eval-elasticsearch.csv', index=False)\n",
    "\n",
    "print(\"Minsearch RAG relevance proportions:\")\n",
    "print(df_eval_minsearch['relevance'].value_counts(normalize=True))\n",
    "print(\"Elasticsearch RAG relevance proportions:\")\n",
    "print(df_eval_es['relevance'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6afaba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RETRIEVAL METRICS ===\n",
      "Minsearch: {'hit_rate': 0.206, 'mrr': 0.10825555555555563}\n",
      "Elasticsearch: {'hit_rate': 0.232, 'mrr': 0.10885634920634935}\n",
      "\n",
      "=== RAG LLM-as-Judge (proportion RELEVANT) ===\n",
      "Minsearch: 0.78\n",
      "Elasticsearch: 0.78\n",
      "\n",
      "All evaluation results saved to CSV in ../data/\n"
     ]
    }
   ],
   "source": [
    "# --- 9. Summary ---\n",
    "\n",
    "print(\"\\n=== RETRIEVAL METRICS ===\")\n",
    "print(\"Minsearch:\", metrics_minsearch)\n",
    "print(\"Elasticsearch:\", metrics_es)\n",
    "print(\"\\n=== RAG LLM-as-Judge (proportion RELEVANT) ===\")\n",
    "print(\"Minsearch:\", (df_eval_minsearch['relevance'] == 'RELEVANT').mean())\n",
    "print(\"Elasticsearch:\", (df_eval_es['relevance'] == 'RELEVANT').mean())\n",
    "\n",
    "print(\"\\nAll evaluation results saved to CSV in ../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb886b",
   "metadata": {},
   "source": [
    "**How to interpret the LLM-as-Judge RAG evaluation output:**\n",
    "\n",
    "The table shows the proportions of answers classified as \"RELEVANT\" or \"PARTLY_RELEVANT\" by the LLM-as-judge for both Minsearch and Elasticsearch RAG pipelines.  \n",
    "- **RELEVANT (0.78):** 78% of generated answers were judged fully relevant to the user's question.\n",
    "- **PARTLY_RELEVANT (0.22):** 22% were judged partially relevant.\n",
    "\n",
    "This indicates that, for ingredient- and attribute-based queries, the majority of answers generated by the RAG pipeline are highly relevant to user needs, regardless of the retrieval backend. This relevance-based evaluation provides a more realistic measure of user experience than strict retrieval metrics alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipe-assistant-P5TWTFE5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
